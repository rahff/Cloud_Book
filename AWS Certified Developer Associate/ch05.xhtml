<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:svg="http://www.w3.org/2000/svg" lang="en" xml:lang="en">
<head>
<title>AWS Certified Developer Associate All-in-One Exam Guide (Exam DVA-C01)</title>
<link href="1260460177.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:c4d348f1-9c3d-457f-b76a-654174c9fde1" name="Adept.expected.resource"/>
</head>
<body>
<section epub:type="chapter">
<h2 class="h2c" id="ch5"><span epub:type="pagebreak" id="page_137"/><span class="chap">CHAPTER <span class="chap1">5</span></span></h2>
<h2 class="h2c1">Elastic Load Balancing and Auto Scaling</h2>
<p class="noindent">In this chapter, you will learn</p>
<p class="bulleta">• Introduction to elastic load balancers</p>
<p class="bulleta">• Types of elastic load balancers</p>
<p class="bulleta">• Listener configuration</p>
<p class="bulleta">• Target groups and targets</p>
<p class="bulleta">• Monitoring load balancers</p>
<p class="bulleta">• Elastic load balancer best practices</p>
<p class="bulleta">• Amazon EC2 Auto Scaling</p>
<p class="bulleta">• Auto Scaling lifecycle</p>
<p class="bulleta">• Launch templates and launch configurations</p>
<p class="bulleta">• Types of scaling</p>
<p class="bulleta">• Monitoring Auto Scaling</p>
<p class="hr"/>
<p class="noindentb">In this chapter you will see how Elastic Load Balancing and Auto Scaling help you to manage high availability and scaling of all your AWS services.</p>
<h3 class="h3" id="ch5lev1sec1">Elastic Load Balancing</h3>
<p class="noindent">The elastic load balancer acts as the single point of entry for your incoming application traffic and distributes it across multiple instances in multiple Availability Zones, which also increases the availability of your application. A listener checks for connection requests from clients and forwards requests to one or more target groups, using the protocol and port that you configure, based on the rules that you define, and you can add one or more listeners to your load balancer. You can add rules that specify different target groups to each listener based on the content of the request, which is called content-based routing.</p>
<p class="indent">Using the protocol and port number that you specified, each target group routes requests to one or more registered targets, such as EC2 instances. You can register a target (i.e., your EC2 instance) with multiple target groups. Health checks can be configured per target group basis, and these are performed on all registered targets of a target group. The diagram in <a href="ch05.xhtml#ch5fig1">Figure 5-1</a> illustrates the basic components of the Elastic Load Balancer. <span epub:type="pagebreak" id="page_138"/>As you can see, each listener contains a default rule, and a second listener contains an additional rule that routes requests to a different target group. The middle target, target-3, is registered with two target groups.</p>
<p class="imagef" id="ch5fig1"><img alt="images" src="f0138-01.jpg"/></p>
<p class="figcaption"><strong>Figure 5-1</strong>   Elastic Load Balancer</p>
<h3 class="h3" id="ch5lev1sec2">Types of Elastic Load Balancers</h3>
<p class="noindent">AWS provides three types of Elastic Load Balancing: Application Load Balancers, Network Load Balancers, and Classic Load Balancers.</p>
<h4 class="h4" id="ch5lev2sec1">Application Load Balancer</h4>
<p class="noindent">An Application Load Balancer functions at the seventh layer of the Open Systems Interconnection (OSI) model, which is also known as the application layer. When a request is received, the listener rules are evaluated by the Application Load Balancer in priority order to determine which rule to apply before selecting a target instance for the rule action from the target group. Even when a target is registered with multiple target groups, the routing is performed independently for each one.</p>
<p class="indent">As and when your application needs change, you can add or remove targets from your load balancer without disrupting your application’s overall flow of requests. Based on your application changes over time, the Elastic Load Balancing scales in or scales out automatically for the majority of workloads. You can monitor the health of your registered targets (i.e., instances) so that the Load Balancer only sends requests to the healthy instance.</p>
<h4 class="h4" id="ch5lev2sec2">Network Load Balancer</h4>
<p class="noindent">A Network Load Balancer can handle millions of requests per second and it functions at the fourth layer of the OSI model, which is the network layer. The Load Balancer selects a target from the target group after it receives a connection request and opens a TCP <span epub:type="pagebreak" id="page_139"/>connection to the selected target on the port specified in the listener configuration. Elastic Load Balancing creates a load balancer node in the Availability Zone when you enable it. Each load balancer node by default distributes the traffic across a single Availability Zone to its registered targets. Each load balancer node distributes traffic across the registered targets in all enabled Availability Zones only if you enable cross-zone load balancing.</p>
<p class="indent">The fault tolerance of your applications increases when you enable multiple Availability Zones for your load balancer and ensure that each target group has at least one target in each enabled Availability Zone. Elastic Load Balancing removes the IP address for the corresponding subnet from DNS when one or more target groups does not have a healthy target in an Availability Zone; however, the load balancer nodes in the other Availability Zones are still available to route traffic. If a client sends requests to the IP address after it is removed from DNS by not honoring time-to-live (TTL) settings, then the requests fail.</p>
<p class="indent">The load balancer selects a target for TCP traffic using a flow hash algorithm based on the protocol, destination IP address, source IP address, destination port, source port, and TCP sequence number. Different sequence numbers and source ports can be routed to different targets using TCP connections from your client. Each individual TCP connection is routed for the life of the connection to a single target. Using the flow hash algorithm based on the protocol, destination IP address, source IP address, destination port, and source port, the load balancer selects a target for User Datagram Protocol (UDP) traffic. A UDP flow is consistently routed to a single target throughout its lifetime, since it has the same source and destination. Different UDP flows can be routed to different targets because of different source IP addresses and ports.</p>
<p class="indent">A network interface is created in each Availability Zone that you enabled by Elastic Load Balancing. This network interface will be used to get a static IP address for each load balancer node in the Availability Zone. You can associate one Elastic IP address per subnet when you create an Internet-facing external load balancer. The target type determines whether you can register targets by instance ID or IP address when you create a target group. The source IP addresses of the clients are preserved and provided to your applications when you register targets by instance ID. The source IP addresses are the private IP addresses of the load balancer nodes when you register targets by IP address.</p>
<p class="indent">You can add and remove targets from your load balancer as your requirements change without disrupting your application’s overall flow of requests. Based on your application usage requirements, the Elastic Load Balancing scales in or scales out the load balancer, and it can also scale to the majority of workloads automatically. You can configure health checks to monitor the health of the registered targets so that the load balancer can send requests only to healthy targets.</p>
<h4 class="h4" id="ch5lev2sec3">Classic Load Balancer</h4>
<p class="noindent">The Classic Load Balancer increases the fault tolerance of your applications by distributing the incoming application traffic across multiple EC2 instances in multiple Availability Zones and detects unhealthy instances and routes traffic only to healthy instances. Your load balancer increases the availability of your application and serves as a single point of contact for clients. You can add and remove instances without disrupting the overall flow of requests to your application from your load balancer as your needs change. <span epub:type="pagebreak" id="page_140"/>Elastic Load Balancing scales your load balancer to the vast majority of workloads automatically as application traffic changes. A listener checks the configured protocol and port for connection requests from clients and forwards the requests to one or more registered instances, and one or more listeners can be added to your load balancer. You can configure health checks and monitor the health of the registered instances so that your load balancer only sends requests to the healthy instances.</p>
<p class="indent">You need to enable cross-zone load balancing on your load balancer to distribute the traffic evenly across all registered instances in all enabled Availability Zones. It is important to keep approximately the same number of instances in each availability zone to ensure that your registered instances are able to handle the request load in each Availability Zone for better fault tolerance. The load balancer distributes the traffic evenly by default across the Availability Zones that you enable for your load balancer. The Classic Load Balancer supports EC2-Classic, TCP, and Secure Sockets Layer (SSL) listeners and also supports sticky sessions using application-generated cookies.</p>
<h3 class="h3" id="ch5lev1sec3"><a id="_idTextAnchor000"/>Elastic Load Balancing Concepts</h3>
<p class="noindent">The following are some of the important components of AWS Elastic Load Balancing concepts that you need to understand to create and manage an ELB.</p>
<h4 class="h4" id="ch5lev2sec4">Load Balancer Subnets</h4>
<p class="noindent">You must specify one public subnet from at least two Availability Zones when you create a load balancer, and you can specify only one public subnet per Availability Zone. You need to make sure each subnet for your load balancer has at least eight free IP addresses and has a minimum of /27 bitmask Classless Inter-Domain Routing (CIDR) block to make sure your load balancer can scale properly. Those IP addresses are used by your load balancer to establish connections with the targets.</p>
<h4 class="h4" id="ch5lev2sec5">Load Balancer Security Groups</h4>
<p class="noindent">The traffic to and from your load balancer is controlled by a security group, which acts as a firewall. The traffic is allowed for both inbound and outbound traffic using the ports and protocols that you configured. The rules for the security groups must allow traffic in both directions on both the listener and the health check ports associated with your load balancer. Whenever you update the health check port for a target group or add a listener to a load balancer, you must review and make sure your security group rules allow traffic on the new port in both inbound and outbound directions.</p>
<h4 class="h4" id="ch5lev2sec6"><a id="_idTextAnchor001"/>Load Balancer States</h4>
<p class="noindent">The states of a load balancer can be one of the following at any point in time:</p>
<p class="bullett">• <strong>Provisioning</strong>   In this state, the load balancer is being set up.</p>
<p class="bulleta">• <strong>Active</strong>   In this state, the load balancer is fully set up and ready to route traffic.</p>
<p class="bulleta">• <strong>Failed</strong>   In this state, the load balancer cannot be set up.</p>
<h4 class="h4" id="ch5lev2sec7"><span epub:type="pagebreak" id="page_141"/>Deletion Protection</h4>
<p class="noindent">You can enable deletion protection to prevent your load balancer from being deleted accidentally. Deletion protection is disabled by default for your load balancer. You must disable deletion protection if it is enabled already before you can delete the load balancer.</p>
<h4 class="h4" id="ch5lev2sec8">Connection Idle Timeout</h4>
<p class="noindent">The load balancer maintains two connections for each request that a client makes. A backend connection is between the load balancer and a target, and a frontend connection is between a client and the load balancer. When no data is sent or received over a frontend connection for a specified time period, then an idle timeout is triggered and the load balancer closes the connection.</p>
<p class="indent">Elastic Load Balancing by default sets the idle timeout value to 60 seconds. The load balancer can close the frontend connection if the target doesn’t send some data at least every 60 seconds while the request is in flight. Increase the length of the idle timeout period as required; for example, to ensure that lengthy operations such as file uploads complete, send at least 1 byte of data before each idle timeout period elapses.</p>
<p class="indent">For backend connections, AWS recommends that you enable the HTTP keep-alive option for your EC2 instances. HTTP keep-alive can be enabled for your EC2 instances in the web server settings, so the load balancer can reuse backend connections until the keep-alive timeout expires. AWS also recommends that you configure the idle timeout of your application to be larger than the idle timeout configured for the load balancer.</p>
<h3 class="h3" id="ch5lev1sec4">Load Balancer Listeners</h3>
<p class="noindent">You must add one or more listeners before you start using your Load Balancer. A listener checks the protocol and port that you configured for connection requests. The listener rule defines how the load balancer routes requests to the targets in one or more target groups.</p>
<h4 class="h4" id="ch5lev2sec9">Listener Configuration</h4>
<p class="noindent">A listener supports HTTP and HTTPS protocols and 1 to 65,535 ports. Your applications can focus on their business logic, since the HTTPS listener is used to offload the encryption and decryption tasks to your load balancer. The HTTPS listener protocol requires at least one SSL certificate on the listener. Application Load Balancers provide native support for WebSockets, and WebSockets can be used with both HTTP and HTTPS listeners.</p>
<p class="indent">HTTP/2 with HTTPS listeners is natively supported by Application Load Balancers. An HTTP/2 connection can be used to send up to 128 requests in parallel. The load balancer converts these into individual HTTP/1.1 requests before distributing them across the healthy targets in your target group. Since HTTP/2 uses frontend connections more efficiently, you notice only fewer connections between the load balancer and clients, and you can’t use the server-push feature of HTTP/2.</p>
<h4 class="h4" id="ch5lev2sec10"><span epub:type="pagebreak" id="page_142"/>Listener Rules</h4>
<p class="noindent">Each listener will have a default rule, but you can add rules if required. Each rule consists of one or more conditions, a priority, and one or more actions. At any time, you can add or edit rules. You define actions for the default rule when you create a listener, and the default rules can’t have conditions. When none of the listener’s rules conditions are met, then the action defined in the default rule is performed.</p>
<p class="bullett">• <strong>Rule Priority</strong>   Each rule will have a priority, and it is evaluated from the lowest value to the highest value in priority order. A nondefault priority can be changed at any time. The evaluation of the default rule is performed last, and the default priority cannot be changed.</p>
<p class="bulleta">• <strong>Rule Actions</strong>   Each rule has an order, a type, and the details required to perform an action.</p>
<p class="bulleta">• <strong>Rule Conditions</strong>   Each rule contains configuration details and type. An action is performed when the conditions of a rule are met.</p>
<p class="bulleta">• <strong>Fixed-Response Actions</strong>   Fixed-response actions can be used to return a custom HTTP response and to drop client requests. The action and the URL of the redirected target are recorded in the access logs when a fixed-response action is taken.</p>
<p class="bulleta">• <strong>Forward Actions</strong>   Forward actions are used to route requests to a particular target group.</p>
<p class="bulleta">• <strong>Redirect Actions</strong>   Redirect actions are used to redirect client requests from one URL to another.</p>
<p class="bulleta">• <strong>Protocol</strong>   You can redirect the HTTP to HTTP protocol, HTTP to HTTPS protocol, and HTTPS to HTTPS protocol, but you cannot redirect the HTTPS to HTTP protocol.</p>
<p class="bulleta">• <strong>Hostname</strong>   A hostname is case-insensitive and includes up to 128 alphanumeric characters.</p>
<p class="bulleta">• <strong>Port</strong>   Port numbers 1 to 65535 are allowed.</p>
<p class="bulleta">• <strong>Path</strong>   The absolute path starts with “/” as the leading character.</p>
<p class="bulleta">• <strong>Query</strong>   These are the query parameters.</p>
<h4 class="h4" id="ch5lev2sec11">Rule Condition Types</h4>
<p class="noindent">The AWS Elastic Load Balancer’s listener rule supports the following condition types to configure rules that route requests based on the conditions set.</p>
<p class="bullett">• <strong>host-header</strong>   Each route request is based on the hostname, which is also known as host-based routing. Multiple domains are supported using a single load balancer.</p>
<p class="bulleta">• <strong>http-header</strong>   Each route request is based on the HTTP headers, which can be standard or custom HTTP header fields.</p>
<p class="bulleta">• <strong>http-request-method</strong>   Each route request is based on the HTTP request and can specify standard or custom HTTP methods.</p>
<p class="bulleta">• <span epub:type="pagebreak" id="page_143"/><strong>path-pattern</strong>   Each route request is based on URL path patterns, which is also known as path-based routing. It is not applied to query parameters, only to the path of the URL.</p>
<p class="bulleta">• <strong>query-string</strong>   This is either key/value pair routing or a value in the query strings.</p>
<p class="bulletb">• <strong>source-ip</strong>   For source IP address routing of each request, the IP address should be in CIDR format. You can use IPv4 or IPv6 addresses; however, wildcard characters are not supported.</p>
<p class="indent">Each rule can contain either zero or one host-header, http-request-method, path-pattern, and source-ip conditions and include zero or more http-header and query-string conditions. Five match evaluations per rule can be specified, and wildcard characters can be included in the match evaluations for host-header, http-header, query-string, and path-pattern conditions, with a limit of five wildcard characters per rule.</p>
<h3 class="h3" id="ch5lev1sec5">Load Balancer Target Groups</h3>
<p class="noindent">A target group routes your requests to one or more registered targets. You specify a target group and conditions when you create each listener rule. Traffic is forwarded to the corresponding target group when a rule condition is met. Different target groups can be created for different types of requests.</p>
<p class="indent">Health check settings for your load balancer are defined on a per target group basis. Unless you override them when you create the target group or modify them later on, each target group uses the default health check settings. The load balancer continually monitors the health of all targets in an Availability Zone that are registered with the target group, based on the rules of the listeners. The load balancer routes requests to healthy registered targets.</p>
<h4 class="h4" id="ch5lev2sec12">Routing Configuration</h4>
<p class="noindent">A load balancer routes requests to its targets by default using the protocol and port number that you configured when you created the target group. When you register a target with the target group, you can also override the port used for routing traffic to a target. The protocols HTTP and HTTPS and ports 1 to 65535 are supported for target groups.</p>
<h4 class="h4" id="ch5lev2sec13">Target Type</h4>
<p class="noindent">The target type can be specified while creating a target group, which determines the type of targets that can be registered with this target group. You cannot change the type of target group after you create it. The following are the target types:</p>
<p class="bullett">• <strong>Instance</strong>   The targets are specified by instance ID, and traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance.</p>
<p class="bulleta">• <strong>IP</strong>   The targets are IP addresses, and you can specify IP addresses that are supported to enable you to register with a target group.</p>
<p class="bulleta">• <strong>Lambda</strong>   The target is a Lambda function, and you can register and invoke a Lambda function when the load balancer receives a request.</p>
<h4 class="h4" id="ch5lev2sec14"><span epub:type="pagebreak" id="page_144"/>Registered Targets</h4>
<p class="noindent">Your load balancer distributes incoming traffic across its healthy registered targets and serves as a single point of contact for your clients. One or more target groups can be registered to each target. Each EC2 instance or IP address can be registered with the same target group multiple times using different ports, which enables the load balancer to route requests to microservices. You can register additional targets with one or more target groups to handle the increased demand of your application. The load balancer starts routing requests to a newly registered target as soon as the target passes the initial health checks and the registration process completes.</p>
<p class="indent">You can deregister targets or any additional targets you registered from your target groups when demand on your application decreases. Deregistering a target does not affect the target and just removes it from your target group. As soon as the target is deregistered, your load balancer stops routing requests to it. Until all the in-flight requests have completed, the target will be in the draining state. When you are ready to resume receiving requests again, reregister the target with a target group.</p>
<h4 class="h4" id="ch5lev2sec15">Deregistration Delay</h4>
<p class="noindent">Elastic Load Balancing stops sending requests to targets that are deregistering and waits 300 seconds by default before completing the deregistration process to allow the in-flight requests to complete. You can change the wait time by updating the deregistration delay value. The initial state of a deregistering target is called draining, and the target state becomes unused after the deregistration delay elapses to complete in-flight requests. If a deregistering target has no in-flight requests and no active connections, the deregistration process completes, without waiting for the deregistration delay to elapse.</p>
<h4 class="h4" id="ch5lev2sec16">Slow Start Mode</h4>
<p class="noindent">A target starts to receive its full share of requests by default, but using slow start mode gives targets time to warm up before the load balancer sends them their full share of requests. When in slow start mode, the load balancer linearly increases the number of requests that it sends to a target. After the slow start duration period elapses, a target exits slow start mode and the load balancer sends the full share of requests.</p>
<h4 class="h4" id="ch5lev2sec17">Sticky Sessions</h4>
<p class="noindent">Sticky sessions help you to route requests to the same target, and it is useful for servers that maintain state information using cookies in order to provide a continuous experience to clients. When a request is received from a client, it is routed to a target and generates a cookie to include in the response to the client. When the next request from the same client contains the cookie, the request goes to the same target group, and the load balancer routes the request to the same target of the cookie. Cookies are encrypted using a rotating key, and the load balancer–generated cookies cannot be decrypted or modified. The duration for the stickiness can be set in seconds, and it can be enabled at the target group level. The sticky session continues if the client sends a request before its duration period expires.</p>
<h3 class="h3" id="ch5lev1sec6"><span epub:type="pagebreak" id="page_145"/>Load Balancer Monitoring</h3>
<p class="noindent">Load balancers need to be monitored continuously to alert on and troubleshoot issues, to target instances, and to analyze traffic patterns. This information can be kept in logs. Amazon CloudWatch can be used to retrieve statistics about data points for your load balancers and targets as an ordered set of time-series data, known as metrics. These metrics can be used to verify whether your system is performing as expected. The access logs contain detailed information about the requests you made to your load balancer, and access logs can be stored in your S3 bucket as log files.</p>
<p class="indent">You can track your HTTP requests using request tracing, and the load balancer adds a header with a trace identifier to each request it receives. Detailed information about application programming interface (API) calls made to the Elastic Load Balancer can be captured using AWS CloudTrail, which stores the log files in your S3 bucket. CloudTrail logs can be used to determine which calls were made, who made the call, when the call was made, and the source IP address where the call came from.</p>
<h3 class="h3" id="ch5lev1sec7">Elastic Load Balancer Best Practices</h3>
<p class="noindent">You need to use multiple Availability Zones in Elastic Load Balancing, as this provides high availability, fault tolerance, resiliency, and easy maintenance cycles because you can take an entire availability zone offline to perform your maintenance and still serve your customers. AWS recommends identifying the target Availability Zones and target group when provisioning the load balancer.</p>
<p class="indent">It is a best practice to configure health checks for the Elastic Load Balancer and monitor all the ports and protocols. Your security group for the Elastic Load Balancer must open only the required ports and protocols because security groups act as another firewall for your AWS resources. Create an Internet-facing Elastic Load Balancer only for external-facing workloads, and create an Internal Load Balancer for your internal needs.</p>
<p class="indent">If you create an Elastic Load Balancer for your web server application, you need to set up health checks for the HTTP/HTTPS protocol instead of creating health checks for the TCP/UDP protocols, which will be used for the Network Load Balancer. Use SSL security certificates to encrypt and decrypt HTTPS connections and terminate the SSL-based encrypted incoming traffic on the ELB instead of terminating the connection on your instances. This offloads SSL processing to AWS, saving you CPU time, costs, and administrative overhead.</p>
<p class="indent">Use connection draining while deleting Elastic Load Balancers and use slow start mode when starting Elastic Load Balancers for critical user applications. You can enable deletion protection to prevent your load balancer from being deleted accidentally, which is disabled by default. As mentioned earlier, load balancers need to be monitored continuously to detect and troubleshoot issues with your load balancers and target instances and to analyze traffic patterns.</p>
<h3 class="h3" id="ch5lev1sec8"><span epub:type="pagebreak" id="page_146"/>Amazon EC2 Auto Scaling</h3>
<p class="noindent">Amazon EC2 Auto Scaling is a service that allows you to maintain the required number of Amazon EC2 instances available to handle the current load for your application. Auto Scaling groups are collections of EC2 instances where you can specify the minimum and maximum number of instances, so your Auto Scaling group never goes below or above these. You can also specify a desired capacity so your group maintains your desired number of instances. Amazon EC2 Auto Scaling can launch or terminate instances based on your application demand using scaling policies.</p>
<p class="indent">As shown in <a href="ch05.xhtml#ch5fig2">Figure 5-2</a>, an Auto Scaling group has a minimum size of two instances, a desired capacity of three instances, and a maximum size of five instances. Based on your specified criteria, the scaling policy defines the number of instances and maintains this within your minimum and maximum numbers.</p>
<p class="imagef" id="ch5fig2"><img alt="images" src="f0146-01.jpg"/></p>
<p class="figcaption"><strong>Figure 5-2</strong>   Auto Scaling group</p>
<h4 class="h4" id="ch5lev2sec18">Advantages of Auto Scaling</h4>
<p class="noindent">Your application achieves better fault tolerance, availability, and cost savings when you add Amazon EC2 Auto Scaling; this also maximizes your benefits on the AWS Cloud. When one of your instances becomes unhealthy, Auto Scaling can detect it and terminate it, and also launch a new instance to replace the unhealthy instance. Amazon EC2 Auto Scaling supports multiple availability zones, so if you configure it to use multiple availability zones and one availability zone becomes unavailable, Auto Scaling can launch instances in another availability zone. This helps your application handle the current traffic demand and can dynamically increase and decrease capacity as needed. This also helps you save money by launching instances only when required and terminating them when the traffic is low.</p>
<p class="indent"><span epub:type="pagebreak" id="page_147"/>Amazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones by launching new instances in the Availability Zone that has fewer instances when enabled. When you change the Availability Zones of your group, or explicitly terminate or detach instances, or an Availability Zone has insufficient capacity, or your spot bid price now has a market price below your bid price, then your Auto Scaling group will become unbalanced between Availability Zones. Amazon EC2 Auto Scaling compensates for all this by rebalancing the Availability Zones by launching new instances before terminating the old ones to avoid performance impact and to keep your application highly available.</p>
<h4 class="h4" id="ch5lev2sec19">The EC2 Auto Scaling Lifecycle</h4>
<p class="noindent">The lifecycle of EC2 instances in an Auto Scaling group starts when an instance is launched and it ends either when you terminate the instance or when the instance is taken out of service and terminated. You are billed for instances as soon as they are launched, including the time that they are not yet in service. <a href="ch05.xhtml#ch5fig3">Figure 5-3</a> shows different transitions between instance states during the lifecycle.</p>
<p class="imagef" id="ch5fig3"><img alt="images" src="f0147-01.jpg"/></p>
<p class="figcaption"><strong>Figure 5-3</strong>   Auto Scaling lifecycle</p>
<p class="indent">The Auto Scaling group launches new EC2 instances and attaches them to the group when you manually increase its size, or when you create a scaling policy to automatically increase based on an increase in demand, or when you set up scaling by schedule to increase the size of the group at a specific time.</p>
<h4 class="h4" id="ch5lev2sec20"><span epub:type="pagebreak" id="page_148"/>Scale Out and In</h4>
<p class="noindent">When a scale-out event occurs, the Auto Scaling group launches the number of EC2 instances defined on the launch configuration. The instance state is initially “pending” until it is fully configured and passes the Amazon EC2 health checks. It is then attached to the Auto Scaling group and the state is changed to “in service” before it is counted toward the desired capacity of the Auto Scaling group. The instances remain in the in-service state until you put them into “standby” state, or you detach the instance from the Auto Scaling group, or the instance fails a required number of health checks, or a scale-in event occurs so it is removed from the Auto Scaling group and terminated.</p>
<p class="indent">A scale-in event occurs when you manually decrease the size of the group, or you create a scaling policy to automatically decrease the size when demand decreases, or you create a schedule to decrease the size of the group at a specific time. When a scale-in event occurs, the Auto Scaling group detaches one or more instances and uses the termination policy to determine which instances to terminate. Instances that are in the process of shutting down and enter the terminating state can’t be put back into service. The instances are completely terminated before entering the terminated state.</p>
<h4 class="h4" id="ch5lev2sec21">Attach and Detach</h4>
<p class="noindent">You can attach or detach an EC2 instance that meets certain criteria from your Auto Scaling group. Your instance is managed as part of the Auto Scaling group after it is attached to it. Your instance can be managed separately from the Auto Scaling group, or you can attach it to a different Auto Scaling group after the instance is detached from the current group.</p>
<h4 class="h4" id="ch5lev2sec22">Lifecycle Hooks</h4>
<p class="noindent">Lifecycle hooks allow you to perform custom actions by pausing instances when an Auto Scaling group launches or terminates an instance. When an instance is paused, it remains in a wait state until either you complete the lifecycle action or the timeout period ends, which is one hour by default. Amazon EC2 Auto Scaling launches one or more instances when it responds to a scale-out event. The instance is initially in the pending state, and then if you have added an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook, the instance moves from the pending to Pending:Wait state. Your instances enter the Pending:Proceed state after the lifecycle action is complete. The instances are attached to your Auto Scaling group after it is fully configured and then finally enter into the in-service state.</p>
<p class="indent">On the other hand, Auto Scaling terminates one or more instances when it responds to a scale-in event. These instances are detached and then enter the terminating state. If you have added an autoscaling:EC2_INSTANCE_TERMINATING lifecycle, the instance moves from the terminating state to the Terminating:Wait state. After completing the lifecycle action, your instance enters the Terminating:Proceed state and is terminated at the end.</p>
<h4 class="h4" id="ch5lev2sec23">Enter and Exit Standby</h4>
<p class="noindent">When you put an instance from the in-service state to the standby state, you can remove the instance from service or troubleshoot or make changes to it before putting it back <span epub:type="pagebreak" id="page_149"/>into service. Instances are not an active part of your application when in the standby state; however, they are managed by the Auto Scaling group.</p>
<h3 class="h3" id="ch5lev1sec9">Launch Templates</h3>
<p class="noindent">A launch template, which is similar to a launch configuration, specifies the Amazon Machine Image (AMI) ID, security groups, a key pair, instance type, and other parameters that you need to launch EC2 instances. You can create multiple versions of a template using launch templates, and a subset of the full set of parameters can be created, which can be reused to create template versions or other templates. You can create a default template with the common configuration parameters and allow the other parameters to be specified as part of another version of the same template. AWS recommends using launch templates instead of launch configurations so that you can use the latest features of Amazon EC2. Launch templates allow you to provision both On-Demand instances and Spot instances across multiple instance types to achieve the desired cost, performance, and scale.</p>
<p class="indent">A launch template can be specified when you update an Auto Scaling group even if it was created using a launch configuration. You can create the template from scratch or create a new version of an existing template to use with an Auto Scaling group, or you can copy the parameters from a launch configuration, running instance, or another template.</p>
<h3 class="h3" id="ch5lev1sec10">Launch Configurations</h3>
<p class="noindent">A launch configuration is an instance configuration template that an Auto Scaling group uses to launch EC2 instances. You can specify instance information, including AMI ID, a key pair, security groups, instance type, and block device mapping when you create a launch configuration. A single launch configuration can be specified in multiple Auto Scaling groups, but only one launch configuration can be used with an Auto Scaling group at a time. Once you have created a launch configuration, it can’t be modified, so you need to create a new launch configuration to update your Auto Scaling group with it.</p>
<p class="indent">You must specify a launch configuration, a launch template, or an EC2 instance whenever you create an Auto Scaling group. Amazon EC2 Auto Scaling automatically creates a launch configuration when you create an Auto Scaling group using an EC2 instance and associates it with the Auto Scaling group.</p>
<h3 class="h3" id="ch5lev1sec11">Auto Scaling Groups</h3>
<p class="noindent">An Auto Scaling group is a logical grouping of multiple Amazon EC2 instances that helps with automatic scaling and management. You can use features like health check replacements and scaling policies in an Auto Scaling group. Automatically scaling and maintaining the number of instances are the core functionalities of Auto Scaling groups.</p>
<p class="indent">You can adjust the size of an Auto Scaling group to meet demand either by using automatic scaling or manually. An Auto Scaling group begins with the desired capacity of instances and maintains it by performing periodic health checks on the instances in the group. Unhealthy <span epub:type="pagebreak" id="page_150"/>instances are terminated by the Auto Scaling group, and Auto Scaling launches another new instance to replace the unhealthy instance. Auto Scaling policies or a schedule scale can be used to increase or decrease the number of instances in your group dynamically and to adjust the desired capacity of the group, as well as to launch or terminate instances as needed between the minimum and maximum capacity values that you specify.</p>
<p class="indent">An Auto Scaling group can launch On-Demand instances, Spot instances, or both when you configure the group to use a launch template. Amazon EC2 can terminate an individual Spot instance as the price or availability of Spot instances changes. The Auto Scaling group attempts to launch a replacement instance when a Spot instance is terminated to maintain the desired capacity for the group. The desired capacity is distributed automatically when you specify multiple Availability Zones, and Auto Scaling maintains the balance across all of your Availability Zones.</p>
<h4 class="h4" id="ch5lev2sec24">Scaling the Size</h4>
<p class="noindent">Scaling is a mechanism that helps you increase or decrease the capacity of the application instance that is serving your traffic. Scaling can be started with an event or an action, also known as scaling action, that triggers an Auto Scaling group to either launch or terminate Amazon EC2 instances. In order to best meet the needs of your applications, Amazon EC2 Auto Scaling provides a number of ways to adjust scaling.</p>
<h3 class="h3" id="ch5lev1sec12">Maintaining the Number of Instances</h3>
<p class="noindent">The Auto Scaling group starts by launching enough EC2 instances to meet the desired capacity, if specified, or the minimum capacity. Auto Scaling performs a periodic health check on running instances, and, as mentioned, when the instance is unhealthy, it terminates that instance and launches a new one. The instance is considered to be unhealthy if you stop or terminate it.</p>
<h4 class="h4" id="ch5lev2sec25">Manual Scaling</h4>
<p class="noindent">You can change the size of an existing Auto Scaling group manually at any time. You can either update the instances that are attached to the Auto Scaling group or update the desired capacity of the Auto Scaling group. Amazon EC2 Auto Scaling manages the process of launching or terminating instances when you change the size of your Auto Scaling group to maintain the new group size.</p>
<h4 class="h4" id="ch5lev2sec26">Scheduled Scaling</h4>
<p class="noindent">Scheduled scaling enables you to set your own schedule based on predictable workload changes, for example, if every week the traffic to your web application starts to increase on Friday, remains high on Saturday, and starts to decrease on Monday.</p>
<p class="indent">You need to create a scheduled action to configure your Auto Scaling group by defining the start and end time and the desired, minimum, and maximum number of instances for the scaling action. These scheduled actions can be either one time only or <span epub:type="pagebreak" id="page_151"/>on a recurring schedule. You can temporarily disable scheduled scaling without deleting your scheduled actions, and you can create a maximum of 125 scheduled actions per Auto Scaling group. A scheduled action must have a unique time, otherwise, the call is rejected with an error message noting the conflict when you attempt to schedule an activity at a time when another scaling activity is already scheduled.</p>
<h4 class="h4" id="ch5lev2sec27">Dynamic Scaling</h4>
<p class="noindent">Dynamic Scaling allows you to scale based on a change in demand and supports target tracking scaling, step scaling, and simple scaling policies. In most cases, any one policy is sufficient to configure your Auto Scaling group to scale out or scale in automatically. You can also set up your Auto Scaling group to have more than one scaling policy, which provides greater flexibility to cover multiple scenarios. But it is possible that each policy could scale in or out at the same time when multiple policies are in force at the same time. In this case, Amazon EC2 Auto Scaling chooses the policy that provides the largest capacity for both scale out and scale in, even when the policies use different criteria for scaling in or out. The intention is to prevent Amazon EC2 Auto Scaling from removing or adding too many instances.</p>
<h5 class="h5">Target Tracking Scaling Policy</h5>
<p class="noindent">The target tracking scaling policy allows you to select a scaling metric and a target value. This works similar to the way you control the temperature of your home using a thermostat, where you select a temperature value and the thermostat automatically maintains this. CloudWatch alarms are used by Amazon EC2 Auto Scaling to create and manage the scaling policy, and Amazon EC2 Auto Scaling calculates the scaling adjustment based on the metric and the target value. The scaling policy removes or adds instances as required to keep the metric at the specified target value and adjusts to changes in the metric due to a changing workload pattern.</p>
<h5 class="h5">Step Scaling Policy</h5>
<p class="noindent">Step adjustments are used to increase or decrease the current capacity of your Auto Scaling group based on a set of scaling adjustments. These adjustments vary based on the size of the alarm breach. The policy continues to respond to additional alarms after a scaling activity is started, even while a scaling activity or health check replacement is in progress. Amazon EC2 Auto Scaling evaluates all the alarms that are breached as it receives the alarm messages, and it does not support cooldown periods for step scaling policies.</p>
<div class="siden">
<p class="imagen"><img alt="Images" class="inlinen" src="tip.jpg"/></p>
<p class="note"><strong>TIP</strong>   AWS recommends using step scaling policies instead of simple scaling policies, even if you have a single scaling adjustment.</p>
</div>
<h5 class="h5">Simple Scaling Policy</h5>
<p class="noindent">Simple scaling increases or decreases the current capacity of the group based on a single scaling adjustment value. Before responding to any additional alarms, the policy waits <span epub:type="pagebreak" id="page_152"/>for the scaling activity or health check replacement to complete and waits until the cooldown period has expired. The cooldown period helps prevent the initiation of additional instances before the effects of previous activities are visible. Your policy is treated as a simple scaling policy if you created your scaling policy before target tracking and step policies were introduced.</p>
<h5 class="h5">Scaling Based on Amazon SQS</h5>
<p class="noindent">Dynamic Scaling allows you to scale based on the response to activity in an Amazon SQS queue, and it is useful for scaling in response to changing conditions, when you don’t know when those conditions will change. An Auto Scaling group processes the messages from an SQS queue to manage the scaling of EC2 instances, and it measures the number of messages in the queue per EC2 instance. Dynamic Scaling uses a target tracking policy to scale based on the custom metric and a set target value, and the CloudWatch alarms invoke the scaling policy.</p>
<h5 class="h5">Deleting a Scaling Policy</h5>
<p class="noindent">You can delete a scaling policy when you no longer need it. When you delete a target tracking scaling policy, any associated CloudWatch alarms are deleted automatically. When you delete a step scaling policy or a simple scaling policy, the underlying alarm action is also deleted, but not the CloudWatch alarm associated with the scaling policy, so you need to delete this manually.</p>
<p class="indent">You can configure a termination policy for the Auto Scaling group to specify which instances to terminate first during a scale-in activity. You can also use Amazon EC2 termination protection to prevent specific instances from being accidentally terminated during automatic scale in.</p>
<h4 class="h4" id="ch5lev2sec28">Cooldown Period</h4>
<p class="noindent">When you define a cooldown period, the Auto Scaling group doesn’t launch or terminate additional instances before the previous scaling activity completes. Only a simple scaling policy supports the cooldown period, not other scaling policies.</p>
<p class="indent">The default is not to wait for the cooldown period, when you manually scale your Auto Scaling group, but you can honor the cooldown period by overriding the default. The Auto Scaling group does not wait for the cooldown period to complete when an instance becomes unhealthy before replacing it.</p>
<h3 class="h3" id="ch5lev1sec13">Monitoring Auto Scaling Groups</h3>
<p class="noindent">Amazon EC2 Auto Scaling performs periodic health checks to identify the instances’ health, but you can also configure Auto Scaling to determine the health status of an instance using Amazon EC2 status checks, Elastic Load Balancing health checks, or custom health checks. Amazon CloudWatch receives data points published by Amazon EC2 Auto Scaling about your Auto Scaling groups. These metrics are an ordered set of time-series data and can be used to verify that your system is performing as expected.</p>
<p class="indent"><span epub:type="pagebreak" id="page_153"/>You can invoke a Lambda function based on the Amazon EC2 Auto Scaling event when your Auto Scaling groups launch or terminate instances or when a lifecycle action occurs. Amazon EC2 Auto Scaling can also send Amazon SNS notifications when your Auto Scaling groups launch or terminate instances. AWS CloudTrail tracks all the API calls made to the Amazon EC2 Auto Scaling and stores the information in log files in your Amazon S3 bucket. These log files can be monitored to verify the activity of your Auto Scaling groups. These logs include which requests were made, where the requests came from, who made the request, the source IP addresses, when the request was made, etc.</p>
<h3 class="h3" id="ch5lev1sec14">Chapter Review</h3>
<p class="noindent">This chapter explained the Elastic Load Balancer and Amazon EC2 Auto Scaling in detail, including all the details a developer needs to build high-performing AWS applications. There are three types of Elastic Load Balancers: Application Load Balancers, Network Load Balancers, and Classic Load Balancers. A load balancer acts as a single point of entry for your application traffic, and it distributes the incoming traffic across multiple targets in multiple availability zones, which increases the availability of your application. An Application Load Balancer functions at the seventh layer of the OSI model, which is the application layer. A Network Load Balancer can handle millions of requests per second and functions at the fourth layer of the OSI model, which is the network layer. You must specify one public subnet from at least two Availability Zones when you create a load balancer, and you can specify only one public subnet per Availability Zone.</p>
<p class="indent">The traffic to and from your load balancer is controlled by a security group, which acts as a firewall. The traffic is allowed for both inbound and outbound traffic using the ports and protocols that you configured. You can enable deletion protection to prevent your load balancer from being deleted accidentally. Elastic Load Balancing by default sets the idle timeout value to 60 seconds, and it closes the connection if no data has been sent or received by the time that the idle timeout period elapses. You must add one or more listeners before you start using your Load Balancer. A listener checks for connection requests, using the protocol and port that you configured. Target groups route your requests to one or more registered targets. You specify a target group and associated conditions when you create each listener rule. Your load balancer distributes incoming traffic across its healthy registered targets and serves as a single point of contact for your clients. Sticky sessions help you route requests to the same target and are useful for servers that maintain state information using cookies in order to provide a continuous experience to clients. Monitoring your load balancers helps you troubleshoot issues with your load balancers and target instances and analyze traffic patterns.</p>
<p class="indent">Amazon EC2 Auto Scaling is a service that allows you to maintain the required number of Amazon EC2 instances available to handle the current load for your application. Auto Scaling groups are collections of EC2 instances, where you can specify the minimum and maximum number of instances. The lifecycle of EC2 instances in an Auto Scaling group starts when an instance is launched and ends either when you terminate <span epub:type="pagebreak" id="page_154"/>the instance or when the Auto Scaling group terminates the instance. When a scale-out event occurs, the Auto Scaling group launches the number of EC2 instances defined on the launch configuration. A scale-in event occurs when you manually decrease the size of the group, or you create a scaling policy to automatically decrease the size when demand decreases, or it can be scheduled to decrease the capacity of the group at a particular time. You can attach or detach an EC2 instance that meets certain criteria from your Auto Scaling group. Lifecycle hooks allow you to perform custom actions by pausing instances when an Auto Scaling group launches or terminates an instance.</p>
<p class="indent">Launch templates allow you to have multiple versions and create a subset of the full set of parameters that you can reuse to create other templates or template versions. A launch configuration is an instance configuration template that an Auto Scaling group uses to launch EC2 instances. AWS recommends using launch templates instead of launch configurations so that you can use the latest features of Amazon EC2. An Auto Scaling group is a logical grouping of multiple Amazon EC2 instances that helps with automatic scaling and management. Scheduled scaling enables you to set your own schedule based on predictable changes in workload. Dynamic Scaling allows you to scale based on changes in demand and supports target tracking scaling, step scaling, and simple scaling policies. When you define the cooldown period, the Auto Scaling group doesn’t launch or terminate additional instances before the previous scaling activity completes. You can delete a scaling policy when you no longer need it. Amazon EC2 Auto Scaling periodically performs health checks to identify the instance’s health, and you can also configure Auto Scaling to determine the health status of an instance using Amazon EC2 status checks, Elastic Load Balancing health checks, or custom health checks to identify any instances that are unhealthy.</p>
<h4 class="h4" id="ch5lev2sec29">Exercises</h4>
<p class="noindent">The following exercises will let you practice using the console to perform various administrative tasks. You need to create an AWS account, as explained earlier, to perform the following exercises. You can use the Free Tier when launching AWS resources, but make sure to terminate them at the end.</p>
<h5 class="h5">Exercise 5-1: Create an Application Load Balancer</h5>
<p class="noindent">This exercise will help you to gain practical experience creating an Application Load Balancer using the AWS Management Console.</p>
<p class="numbert"><strong>1.</strong> Open the Amazon EC2 console at <a href="https://console.aws.amazon.com/ec2/">https://console.aws.amazon.com/ec2/</a>.</p>
<p class="number"><strong>2.</strong> You need to choose a region for your load balancer on top of the navigation bar and select the same region where your EC2 instances are.</p>
<p class="number"><strong>3.</strong> Below LOAD BALANCING on the navigation pane, choose Load Balancers.</p>
<p class="number"><strong>4.</strong> Then choose Create Load Balancer.</p>
<p class="number"><strong>5.</strong> Since we are creating an Application Load balancer, select Application Load Balancer and choose Create.</p>
<h5 class="h5"><span epub:type="pagebreak" id="page_155"/>Exercise 5-2: Configure Your Load Balancer and Listener</h5>
<p class="noindent">This exercise provides practical experience creating a listener using the AWS Management Console.</p>
<p class="numbert"><strong>1.</strong> Under Name, type the appropriate name for your Application Load Balancer. Your Application Load Balancer name must be unique.</p>
<p class="number"><strong>2.</strong> You can keep the default values for the Scheme and IP Address type.</p>
<p class="number"><strong>3.</strong> By default, the listener accepts HTTP traffic on port 80, so keep this.</p>
<p class="number"><strong>4.</strong> For Availability Zones, select the VPC that you used for your EC2 instances and then select the public subnet for that Availability Zone.</p>
<p class="number"><strong>5.</strong> Choose Next: Configure Security Settings.</p>
<h5 class="h5">Exercise 5-3: Configure a Security Group for Your Load Balancer</h5>
<p class="noindent">This exercise provides practical experience configuring a security group using the AWS Management Console.</p>
<p class="numbert"><strong>1.</strong> Choose Create A New Security Group.</p>
<p class="number"><strong>2.</strong> Type a name and description for the security group, or you can keep the defaults.</p>
<p class="number"><strong>3.</strong> When ready, choose Next to navigate to the Configure Routing page.</p>
<h5 class="h5">Exercise 5-4: Configure Your Target Group</h5>
<p class="noindent">This exercise provides practical experience configuring a target group using the AWS Management Console.</p>
<p class="numbert"><strong>1.</strong> For Target Group, you can keep the default value, and then select New Target Group.</p>
<p class="number"><strong>2.</strong> Provide an appropriate name for the new target group.</p>
<p class="number"><strong>3.</strong> You can keep the default target type, which is an instance, HTTP as the protocol, and 80 for the port.</p>
<p class="number"><strong>4.</strong> You can keep the default values for Health Checks.</p>
<p class="number"><strong>5.</strong> When ready, choose Next to move to the Register Targets page.</p>
<h5 class="h5">Exercise 5-5: Register Targets with Your Target Group</h5>
<p class="noindent">This exercise provides practical experience registering targets using the AWS Management Console.</p>
<p class="numbert"><strong>1.</strong> When you are on the Register Targets page, select one or more instances as required.</p>
<p class="number"><strong>2.</strong> Here you can keep the default port as 80 and then choose Add To Registered Targets.</p>
<p class="number"><strong>3.</strong> After you have finished selecting instances, choose Next to navigate to the Review page.</p>
<h5 class="h5"><span epub:type="pagebreak" id="page_156"/>Exercise 5-6: Create and Test Your Load Balancer</h5>
<p class="noindent">This exercise provides practical experience creating and testing a load balancer using the AWS Management Console.</p>
<p class="numbert"><strong>1.</strong> When you are on the Review page, if everything looks okay, choose Create.</p>
<p class="number"><strong>2.</strong> Choose Close after you are notified that your load balancer was created successfully.</p>
<p class="number"><strong>3.</strong> Below LOAD BALANCING on the navigation pane, you can choose Target Groups.</p>
<p class="number"><strong>4.</strong> Select the newly created target group.</p>
<p class="number"><strong>5.</strong> Verify that your instances are ready on the Targets tab.</p>
<p class="number"><strong>6.</strong> After the status of at least one instance is healthy, you can test your load balancer.</p>
<p class="number"><strong>7.</strong> Below LOAD BALANCING on the navigation pane, choose Load Balancers.</p>
<p class="number"><strong>8.</strong> Select the newly created load balancer.</p>
<p class="number"><strong>9.</strong> From the Description tab, copy the DNS name of the load balancer and paste it into an Internet-connected web browser.</p>
<p class="number1"><strong>10.</strong> If everything is working, the browser displays the default page of your server.</p>
<h5 class="h5">Exercise 5-7: Delete Your Load Balancer</h5>
<p class="noindent">As soon as your load balancer becomes available, you are billed for each hour or partial hour, so delete it if you no longer need it.</p>
<p class="numbert"><strong>1.</strong> Open the Amazon EC2 console at <a href="https://console.aws.amazon.com/ec2/">https://console.aws.amazon.com/ec2/</a>.</p>
<p class="number"><strong>2.</strong> Below LOAD BALANCING on the navigation pane, choose Load Balancers.</p>
<p class="number"><strong>3.</strong> Select the checkbox for the load balancer, and then choose Actions and select Delete from drop-down list to delete your load balancer.</p>
<p class="number"><strong>4.</strong> When prompted for confirmation, choose Yes, Delete, which deletes your load balancer permanently.</p>
<h4 class="h4" id="ch5lev2sec30">Questions</h4>
<p class="noindent">The following questions will help you gauge your understanding of Elastic Load Balancing and Auto Scaling in this chapter. Read all the answers carefully because there might be more than one correct answer. Choose the best responses for each question.</p>
<p class="numbert"><strong><a href="ch05.xhtml#rch05qa1" id="ch05qa1">1.</a></strong> What Load Balancers are available in AWS? (Choose three.)</p>
<p class="alphau"><strong>A.</strong> Application Load Balancer</p>
<p class="alphau"><strong>B.</strong> Network Load Balancer</p>
<p class="alphau"><strong>C.</strong> Global Load Balancer</p>
<p class="alphau"><strong>D.</strong> Classic Load Balancer</p>
<p class="number"><strong><a href="ch05.xhtml#rch05qa2" id="ch05qa2">2.</a></strong> <span epub:type="pagebreak" id="page_157"/>What are lifecycle hooks?</p>
<p class="alphau"><strong>A.</strong> It allows you to take actions before an instance goes into service or before it is terminated.</p>
<p class="alphau"><strong>B.</strong> It is a collection of EC2 instances used for Dynamic Scaling and fleet management.</p>
<p class="alphau"><strong>C.</strong> It is a template that your EC2 Auto Scaling group uses to launch EC2 instances based on the configuration.</p>
<p class="alphau"><strong>D.</strong> It is a failed user-configured ELB health check, or the hardware has become impaired.</p>
<p class="number"><strong><a href="ch05.xhtml#rch05qa3" id="ch05qa3">3.</a></strong> Your customer wants to use a single Application Load Balancer to handle both HTTP and HTTPS requests. Is this possible?</p>
<p class="alphau"><strong>A.</strong> No, only HTTP port 80 listener can be added to any single Application Load Balancer.</p>
<p class="alphau"><strong>B.</strong> No, only HTTPS port 443 listener can be added to any single Application Load Balancer.</p>
<p class="alphau"><strong>C.</strong> No, both HTTP port 80 and HTTPS port 443 listeners cannot be added to a single Application Load Balancer.</p>
<p class="alphau"><strong>D.</strong> Yes, both HTTP port 80 and HTTPS port 443 listeners can be added to a single Application Load Balancer.</p>
<p class="number"><strong><a href="ch05.xhtml#rch05qa4" id="ch05qa4">4.</a></strong> Your developer tries to use EC2 Auto Scaling groups to span multiple AWS regions for high availability but he is getting an error. Is it possible to configure an EC2 Auto Scaling Group to span across different AWS regions?</p>
<p class="alphau"><strong>A.</strong> Yes, EC2 Auto Scaling groups can span across multiple AWS regions.</p>
<p class="alphau"><strong>B.</strong> No, EC2 Auto Scaling cannot span across AWS regions; however, it can span across multiple availability zones.</p>
<p class="alphau"><strong>C.</strong> Yes, EC2 Auto Scaling groups can span across AWS regions only if this is defined in the launch configuration.</p>
<p class="alphau"><strong>D.</strong> Yes, EC2 Auto Scaling groups can span across AWS regions only if the lifecycle hooks are defined.</p>
<p class="number"><strong><a href="ch05.xhtml#rch05qa5" id="ch05qa5">5.</a></strong> Which of the following about the Network Load Balancer are true? (Choose all that apply.)</p>
<p class="alphau"><strong>A.</strong> Network Load Balancer supports both TCP and UDP (layer 4) load balancing.</p>
<p class="alphau"><strong>B.</strong> Network Load Balancer can handle millions of requests per second and provides extremely low latency.</p>
<p class="alphau"><strong>C.</strong> Network Load Balancer supports TLS termination and preserves the source IP of the clients.</p>
<p class="alphau"><strong>D.</strong> Network Load Balancer supports long-running connections for WebSocket-type applications.</p>
<p class="number"><strong><a href="ch05.xhtml#rch05qa6" id="ch05qa6">6.</a></strong> <span epub:type="pagebreak" id="page_158"/>The instances in your application layer need to be load-balanced with your database layer without allowing any external Internet traffic. Can you use Network Load Balancer to set up internal load balancers?</p>
<p class="alphau"><strong>A.</strong> No, the Network Load Balancer can only be set up as an Internet-facing external load balancer.</p>
<p class="alphau"><strong>B.</strong> Yes, Network Load Balancer can be set up as either an Internet-facing external load balancer or an internal load balancer.</p>
<p class="alphau"><strong>C.</strong> No, only Application Load Balancer can be set up as an internal load balancer.</p>
<p class="alphau"><strong>D.</strong> No, only Classic Load Balancer can be set up as an internal load balancer.</p>
<p class="number"><strong><a href="ch05.xhtml#rch05qa7" id="ch05qa7">7.</a></strong> Which of the following is supported in Application Load Balancer? (Choose three.)</p>
<p class="alphau"><strong>A.</strong> WebSockets and Secure WebSockets are supported in Application Load Balancer.</p>
<p class="alphau"><strong>B.</strong> Request tracing is supported and enabled by default in Application Load Balancer.</p>
<p class="alphau"><strong>C.</strong> HTTP/2 support is enabled natively on an Application Load Balancer and can connect over TLS.</p>
<p class="alphau"><strong>D.</strong> Application Load Balancer supports layer 4 load balancing.</p>
<p class="number"><strong><a href="ch05.xhtml#rch05qa8" id="ch05qa8">8.</a></strong> Your development team is using Elastic Load Balancer, and before implementing it in production they want to know whether there is an SLA for load balancers. What do you tell them?</p>
<p class="alphau"><strong>A.</strong> Elastic Load Balancing guarantees a monthly availability of at least 99.99 percent for Classic, Application, or Network Load Balancers.</p>
<p class="alphau"><strong>B.</strong> Elastic Load Balancing does not guarantee any SLA for your Classic, Application, or Network Load Balancers.</p>
<p class="alphau"><strong>C.</strong> Elastic Load Balancing guarantees a monthly availability of at least 98.99 percent for your Application and Network Load Balancers.</p>
<p class="alphau"><strong>D.</strong> Elastic Load Balancing guarantees a monthly availability of at least 99.99 percent only for your Network Load Balancers.</p>
<p class="number"><strong><a href="ch05.xhtml#rch05qa9" id="ch05qa9">9.</a></strong> You are configuring Amazon EC2 Auto Scaling and plan to use the health checks that work with your Application Load Balancers and Network Load Balancers. If any target group associated with it becomes unhealthy, will an instance be marked as unhealthy?</p>
<p class="alphau"><strong>A.</strong> No, Amazon EC2 Auto Scaling does not work with the health check feature of Application Load Balancers and Network Load Balancers. You need to configure a different health check for EC2 Auto Scaling.</p>
<p class="alphau"><strong>B.</strong> Yes, Amazon EC2 Auto Scaling works with the health check feature of Application Load Balancers, but not with Network Load Balancers.</p>
<p class="alphau"><strong>C.</strong> Yes, Amazon EC2 Auto Scaling works with the health check feature of Network Load Balancers, but not with Application Load Balancers.</p>
<p class="alphau"><strong>D.</strong> Yes, Amazon EC2 Auto Scaling works with the health check feature of both Application Load Balancers and Network Load Balancers.</p>
<p class="number1"><strong><a href="ch05.xhtml#rch05qa10" id="ch05qa10">10.</a></strong> <span epub:type="pagebreak" id="page_159"/>Your manager asked you to set up an EC2 Auto Scaling group. Do you need to install CloudWatch agents manually or will the Amazon EC2 Auto Scaling group install the agents automatically?</p>
<p class="alphau"><strong>A.</strong> It is automatically installed on EC2 instances if your AMI contains a CloudWatch agent when you create an EC2 Auto Scaling group.</p>
<p class="alphau"><strong>B.</strong> You need to manually install this using the recommended <span class="code">yum</span> command on each instance.</p>
<h4 class="h4" id="ch5lev2sec31">Answers</h4>
<p class="numbert"><strong><a href="ch05.xhtml#ch05qa1" id="rch05qa1">1.</a> A, B, D.</strong> You can select Application Load Balancer when you need to load-balance HTTP requests, and for extreme performance/low latency applications, you can use Network Load Balancer. If your application needs HTTP, HTTPS (Secure HTTP), SSL (Secure TCP), or TCP, you can use Classic Load Balancer.</p>
<p class="number"><strong><a href="ch05.xhtml#ch05qa2" id="rch05qa2">2.</a> A.</strong> It allows you to perform custom actions by pausing instances when an Auto Scaling group launches or terminates the instance. When an instance is paused, it remains in a wait state until either you complete the lifecycle action or the timeout period ends, which is one hour by default.</p>
<p class="number"><strong><a href="ch05.xhtml#ch05qa3" id="rch05qa3">3.</a> D.</strong> Yes, both HTTP port 80 and HTTPS port 443 listeners can be added to a single Application Load Balancer.</p>
<p class="number"><strong><a href="ch05.xhtml#ch05qa4" id="rch05qa4">4.</a> B.</strong> No, EC2 Auto Scaling cannot span across AWS regions; however, it can span across multiple Availability Zones.</p>
<p class="number"><strong><a href="ch05.xhtml#ch05qa5" id="rch05qa5">5.</a> A, B, C, D.</strong> Network Load Balancer supports both TCP and UDP (layer 4) load balancing, and it can handle millions of requests per second and provides extremely low latency. Also, it supports TLS termination and preserves the source IP of the clients in addition to supporting long-running connections for WebSocket-type applications.</p>
<p class="number"><strong><a href="ch05.xhtml#ch05qa6" id="rch05qa6">6.</a> B.</strong> Yes, Network Load Balancer can be set up as either an Internet-facing external load balancer or an internal load balancer.</p>
<p class="number"><strong><a href="ch05.xhtml#ch05qa7" id="rch05qa7">7.</a> A, B, C.</strong> WebSockets and Secure WebSockets and request tracing are supported in addition to HTTP/2 support, which is enabled natively on an Application Load Balancer and can connect over TLS.</p>
<p class="number"><strong><a href="ch05.xhtml#ch05qa8" id="rch05qa8">8.</a> A.</strong> Yes, Amazon Elastic Load Balancing guarantees a monthly availability of at least 99.99 percent for your Classic, Application, or Network Load Balancers.</p>
<p class="number"><strong><a href="ch05.xhtml#ch05qa9" id="rch05qa9">9.</a> D.</strong> Yes, Amazon EC2 Auto Scaling works with the health check feature of both Application Load Balancers and Network Load Balancers.</p>
<p class="number1"><strong><a href="ch05.xhtml#ch05qa10" id="rch05qa10">10.</a> A.</strong> Yes, it is automatically installed on EC2 instances if your AMI contains a CloudWatch agent when you create an EC2 Auto Scaling group.</p>
<h3 class="h3" id="ch5lev1sec15"><span epub:type="pagebreak" id="page_160"/>Additional Resources</h3>
<p class="bullett">• <strong>UDP Load Balancing</strong>   There is no place like the official AWS documentation to get the latest and most up-to-date information about all the AWS services and updates to existing features. The Network Load Balancer handles tens of millions of requests per second while maintaining high throughput and supports resource-based and tag-based permissions and the ability to load-balance UDP traffic. This blog explains in detail the steps to configure the same load balancer for both TCP and UDP traffic instead of maintaining a fleet of proxy servers to ingest UDP traffic.</p>
<p class="bulletc"><img alt="images" src="p0160-01.jpg"/></p>
<p class="bulleta">• <strong>Dynamic Scaling</strong>   This blog explains the detailed steps to launch and manage an entire fleet of EC2 Spot instances with one request using the RequestSpotFleet API, which helps achieve cost savings.</p>
<p class="bulletc"><img alt="images" src="p0160-02.jpg"/></p>
<p class="bulleta">• <strong>Advanced Request Routing</strong>   This blog explains the steps you need to write rules and route traffic based on standard and custom HTTP headers and methods, the query string, and the source IP address using AWS Application Load Balancers, which helps in eliminating the need for a proxy fleet for routing and to block unwanted traffic at the load balancer level.</p>
<p class="bulletc"><img alt="images" src="p0160-03.jpg"/></p>
<p class="bulleta">• <strong>Auto Scale Amazon EC2 Spot Instances</strong>   This blog explains how to architect the workload properly to handle the two-minute warning to avoid outages or disconnections by avoiding state or end-user stickiness to specific instances, fault tolerance, decoupling with queueing mechanisms, and keeping compute and storage decoupled.</p>
<p class="bulletc"><img alt="images" src="p0160-04.jpg"/></p>
<p class="bulleta">• <strong>TLS Termination</strong>   This blog provides the steps for creating a Network Load Balancer and making use of TLS connections that terminate at a Network Load Balancer, as well as freeing your backend servers from the compute-intensive work of encrypting and decrypting all of your traffic.</p>
<p class="bulletc"><img alt="images" src="p0160-05.jpg"/></p>
</section>
</body>
</html>