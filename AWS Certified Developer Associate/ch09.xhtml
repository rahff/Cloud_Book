<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:svg="http://www.w3.org/2000/svg" lang="en" xml:lang="en">
<head>
<title>AWS Certified Developer Associate All-in-One Exam Guide (Exam DVA-C01)</title>
<link href="1260460177.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:c4d348f1-9c3d-457f-b76a-654174c9fde1" name="Adept.expected.resource"/>
</head>
<body>
<section epub:type="chapter">
<h2 class="h2c" id="ch9"><span epub:type="pagebreak" id="page_249"/><span class="chap">CHAPTER <span class="chap1">9</span></span></h2>
<h2 class="h2c1">Amazon EBS, Amazon EFS, and Amazon S3 Glacier</h2>
<p class="noindent">In this chapter, you will learn</p>
<p class="bulleta">• Amazon Elastic Block Store</p>
<p class="bulleta">• Working with EBS volume</p>
<p class="bulleta">• Amazon EBS snapshots</p>
<p class="bulleta">• Amazon EBS encryption</p>
<p class="bulleta">• Amazon EBS best practices</p>
<p class="bulleta">• Amazon Elastic File System</p>
<p class="bulleta">• EFS encryption</p>
<p class="bulleta">• Lifecycle policy</p>
<p class="bulleta">• Amazon EFS best practices</p>
<p class="bulleta">• Amazon S3 Glacier</p>
<p class="bulleta">• Vault, archives, and jobs</p>
<p class="bulleta">• Notification configuration</p>
<p class="bulleta">• Logging and monitoring</p>
<p class="hr"/>
<p class="noindentb">This chapter will show how Amazon EBS, Amazon EFS, and Glacier help you with a variety of storage requirements.</p>
<h3 class="h3" id="ch9lev1sec1">Amazon Elastic Block Store</h3>
<p class="noindent">Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes, which behave like raw, unformatted block devices and are used for EC2 instances. You pay only for what you use, and you can mount a single volume as a device or mount multiple volumes on the same instance; however, each volume can be attached to only one instance at a time. A file system can be created on top of the EBS volume, or it can be used like a hard drive. The volume configuration, which can be changed dynamically, is attached to an instance. EBS volumes can be attached to any running instance in the same Availability Zone, which persists independently from the life of that instance. AWS recommends using Amazon EBS when you need to access data quickly and store it long term and for <span epub:type="pagebreak" id="page_250"/>file systems; databases; or raw, unformatted, block-level storage. Amazon EBS is well suited for random reads and writes and for long, continuous reads and writes.</p>
<p class="indent">EBS volumes are specific to an Availability Zone, and you can attach them to an instance in the same Availability Zone. If you need to make an EBS volume available to another Availability Zone in the same region or to another Availability Zone in different region, you need to create a snapshot from the existing Availability Zone, copy the snapshot to the target location, and restore the snapshot to a new volume in the target Availability Zone. EBS volumes support live configuration changes, like modifying the volume type, volume size, and input/output operations per second (IOPS) capacity without service interruption. Amazon EBS provides solid-state drive (SSD)–backed volumes, which are optimized for transactional workloads with frequent read/write operations using small input/output (I/O) size. Hard disk drive (HDD)–backed volumes are optimized for large streaming workloads where throughput is a better performance measure than IOPS. Amazon EBS offers following volume types:</p>
<p class="bullett">• <strong>General-purpose SSD (gp2)</strong>   offers performance from IOPS/GiB and burst performance up to 3,000 IOPS and is ideal for a wide range of use cases like small- and medium-size databases, boot volumes, and development and test environments. A gp2 volume size ranges from 1GiB to 16TiB and delivers 90 percent of the provisioned performance 99 percent of the time.</p>
<p class="bulleta">• <strong>Provisioned IOPS SSD (io1)</strong>   volumes offer up to 64,000 IOPS and a throughput of 1,000 MiB/s and scale to tens of thousands of IOPS per EC2 instance. They are designed to meet the needs of I/O-intensive workloads, particularly database workloads, which need strong consistency and storage performance. An io1 volume size ranges from 4GiB to 16TiB and delivers the provisioned IOPS performance 99.9 percent of the time.</p>
<p class="bulleta">• <strong>Throughput-optimized HDD (st1)</strong>   volumes offers low-cost magnetic storage that is ideal for sequential, large workloads such as Extract, Transform, Load (ETL), Amazon Elastic MapReduce (EMR), log processing, and data warehouses. The st1 volumes use a burst-bucket model for performance and are designed to support frequently accessed data involving large, sequential I/O.</p>
<p class="bulleta">• <strong>Cold HDD (sc1)</strong>   volumes offer low-cost magnetic storage that is ideal for sequential, cold-data workloads when you require infrequent access and want to save costs. The sc1 uses a burst-bucket model for performance.</p>
<p class="bulletb">• <strong>Magnetic (standard)</strong>   volumes are backed by magnetic drives and are suited for infrequently accessed workloads and low-cost storage. It delivers 100 IOPS on average, and the size ranges from 1GiB to 1TiB.</p>
<p class="indent">The burst-bucket level for gp2, st1, and sc1 volumes can be monitored using the EBS BurstBalance metric available in Amazon CloudWatch, and it shows the percentage of I/O credits for gp2 volumes or throughput credits for st1 and sc1 volumes remaining in the burst bucket. An alarm can be set up in CloudWatch to notify you when the BurstBalance value falls to a specific level. You need to create EBS volumes as encrypted volumes <span epub:type="pagebreak" id="page_251"/>for compliance, regulatory, and audit requirements. When you attach an encrypted EBS volume to an instance, the data stored on the volume, and snapshots are encrypted at rest and the disk I/O is encrypted in transit. The encryption occurs on the servers that provide encryption of data-in-transit from EC2 instances to EBS storage. Point-in-time snapshots of EBS volumes can be created and stored on Amazon S3. Snapshots protect data and can be used to create as many EBS volumes as you like. These snapshots can be copied across AWS Availability Zones and regions. Amazon CloudWatch provides metrics like latency, bandwidth, average queue length, and throughput to monitor the volume performance.</p>
<p class="indent">EBS volumes are automatically replicated within the zone to prevent any data loss due to failure of any single hardware component. When you attach it to an EC2 instance, it appears as a native block device, same as a hard drive or any other physical device. The instance can format the EBS volume like a local drive with a file system, such as ext3, before installing the applications. You can attach multiple EBS volumes to a single instance; however, a single EBS volume can be attached to only one instance at a time. You can stripe data across the volumes when you have multiple volumes attached to a device for increased I/O and throughput performance. An EBS volume can persist independently from the life of an instance, and you incur volume usage charges as long as the data persists. Based on the Delete On Termination flag, either the EBS volume is deleted automatically when the instance is terminated or the EBS volumes persists by detaching automatically when the instance is terminated. The detached volume can then be reattached to a new instance to enable quick recovery. You can stop and restart an instance without affecting the data when you are using an EBS-backed instance, and the volume remains attached during the stop-and-start cycle. The data persists on the volume until the volume is deleted explicitly, and the deleted EBS volumes are overwritten with zeroes.</p>
<h3 class="h3" id="ch9lev1sec2">Create an Amazon EBS Volume</h3>
<p class="noindent">You can create an Amazon EBS volume in the same Availability Zone where you want to attach to an EC2 instance as an encrypted EBS volume, but it can only be attached to selected instance types. An Amazon EBS volume can be restored using snapshot ID, which is stored in Amazon S3, and you should have snapshot access permissions. The preferred backup tool on Amazon EC2 is EBS snapshots because of their speed, cost, and convenience. You can re-create the volume state at a specific point in the past with all data intact when restoring it from a snapshot. You can restore a volume to copy data across regions, to retrieve specific files, to create test environments, and to transfer them to another attached volume or to replace a damaged or corrupted volume. After a volume is created from a snapshot, you don’t need to wait for all of the data to transfer from Amazon S3 to your EBS volume before accessing the volume and all its data because it loads in the background. The volume immediately downloads the requested data from Amazon S3 when your instance accesses data that hasn’t yet been loaded and then continues loading the rest of the volume data in the background.</p>
<h3 class="h3" id="ch9lev1sec3"><span epub:type="pagebreak" id="page_252"/>Working with Amazon EBS Volume</h3>
<p class="noindent">An Amazon EBS volume is exposed as a block device after you attach it to your instance. The volume can be formatted with any file system and then mounted. When you make the EBS volume available for use, it can be accessed as you access any other volume. The data is written directly to the EBS volume and is transparent to applications using the device. Snapshots can be taken from your EBS volume for backup purposes or to use as a baseline. The descriptive information, including its size, volume type, whether the volume is encrypted, which master key was used to encrypt the volume, and the specific instance to which the volume is attached, can be viewed. The available disk space for the EBS volume from the Linux operating system can be viewed using the following:</p>
<p class="imagep"><img alt="images" src="p0252-01.jpg"/></p>
<h4 class="h4" id="ch9lev2sec1">Detach an Amazon EBS Volume</h4>
<p class="noindent">An Amazon EBS volume can be detached from an instance explicitly or when you terminate it. You need to unmount the volume from the instance if it is running. Before you can detach the volume, you need to stop the instance if an EBS volume is the root device. The volume might not get the same mount point when you reattach a volume that you detached without unmounting it. If there were writes to the volume in progress when it was detached, then the data on the volume might be out of sync.</p>
<div class="siden">
<p class="imagen"><img alt="Images" class="inlinen" src="examtip.jpg"/></p>
<p class="note"><strong>EXAM TIP</strong>   If the storage amount exceeds the limit of the AWS Free Tier, then you will be charged for volume storage even after detaching the volume. If you no longer need it, you can delete the volume to avoid incurring further charges.</p>
</div>
<h4 class="h4" id="ch9lev2sec2">Delete an Amazon EBS Volume</h4>
<p class="noindent">You can delete an Amazon EBS volume if you no longer need it. After deleting the volume, its data is not available, nor can it be attached to any instance. You can take a snapshot of the volume before deletion, which can be used to re-create the volume later.</p>
<h3 class="h3" id="ch9lev1sec4">Monitor Amazon EBS Volumes</h3>
<p class="noindent">You can use volume status checks to better understand, manage, and track potential inconsistencies in the data on an Amazon EBS volume, which provides you with the information you need to determine whether the volumes are impaired. These checks automatically run every five minutes and return a pass or fail status. The status of the <span epub:type="pagebreak" id="page_253"/>volume is OK if all checks pass. The status of the volume is impaired if a check fails. If the status reports insufficient data, the checks may still be in progress on the volume.</p>
<p class="imaget1"><img alt="images" src="t0253-01.jpg"/></p>
<p class="indent">By default the I/O is disabled from the attached EC2 instance when the Amazon EBS determines that a volume’s data is potentially inconsistent to prevent data corruption. The next volume status check fails once I/O is disabled, and the volume status is impaired. An event is generated so you can resolve the impaired status of the volume by enabling I/O for the volume. AWS allows you to enable I/O to let your instances use the volume, and you can run a consistency check using the <span class="code">fsck</span> command before enabling I/O. If the consistency of a volume is not a concern, you can override the default behavior by configuring the volume to automatically enable I/O, which causes the volume status checks to continue to pass. In addition, an event is generated to let you know that the volume was determined to be potentially inconsistent so you can check the volume’s consistency or replace it at a later time. The I/O performance status check is not valid for Throughput-Optimized HDD (st1), General-Purpose SSD (gp2), Magnetic (standard), or Cold HDD (sc1) volumes and is only available for io1 volumes that are attached to an instance. CloudWatch collects this data every five minutes even though the I/O performance status check is performed once a minute.</p>
<h3 class="h3" id="ch9lev1sec5">Amazon EBS Snapshots</h3>
<p class="noindent">You can create backups of your Amazon EBS volumes using snapshots, which are stored in Amazon S3 in multiple Availability Zones. You can take snapshots even when the volume does not need to be attached to a running instance. You can create a snapshot of a volume to use it as a baseline, and it can be used to create multiple new EBS volumes or moved across Availability Zones. The snapshots you create from an encrypted EBS volume are automatically encrypted. A new EBS volume that is created from a snapshot is an exact copy of its original volume at the time the snapshot was taken. When you restore EBS volumes from an encrypted snapshot, it is automatically encrypted. You can create <span epub:type="pagebreak" id="page_254"/>a duplicate volume in another Availability Zone by specifying it. You can share your snapshots with specific AWS accounts or even make them public. You incur Amazon S3 charges based on the volume’s total size while creating the snapshots. The successive incremental snapshots are charged only for any additional data stored beyond the volume’s original size. A snapshot uses an incremental backup, in that only the blocks on the volume that have changed after your most recent snapshot are stored. For example, if you have a volume with 215GiB of data but only 3.6GiB of data have changed from the time of the last snapshot, only the 3.6GiB of the modified data are written to Amazon S3. Even though snapshots are incremental, the snapshot deletion process retains only the most recent snapshot in order to restore the volume. To help categorize and manage your volumes and snapshots, tag them with metadata.</p>
<p class="indent">An Amazon EBS volume allows you to take point-in-time snapshots to back up the data to Amazon S3. Snapshots are incremental because only the blocks that have changed since your most recent snapshot are backed up to minimize the time and save storage costs by avoiding duplicate data. Only the data that is unique to that snapshot is removed when you delete a snapshot. Each snapshot contains all of the information that was captured from the moment when the snapshot was taken, and the snapshot information is required to restore your data to a new EBS volume. When you restore a snapshot to create an EBS volume, the new volume begins as an exact replica of the original volume that was used to create that snapshot. You can begin using the replicated volume immediately while it loads data in the background. If you try to access data that hasn’t been loaded yet, it is downloaded immediately from Amazon S3 and then the rest of the volume’s data is loaded in the background.</p>
<h4 class="h4" id="ch9lev2sec3">Multivolume Snapshots</h4>
<p class="noindent">You can create snapshot backups of critical workloads like a large database or a file system that spans across multiple EBS volumes. Multivolume snapshots can be taken across multiple EBS volumes attached to an EC2 instance, which allows you to take exact point-in-time, crash-consistent, and data-coordinated snapshots. Snapshots are automatically taken across multiple EBS volumes without requiring you to stop your instance or coordinate between volumes to ensure crash consistency. CloudWatch events can be used to track the status of your EBS snapshots.</p>
<h4 class="h4" id="ch9lev2sec4">Delete an Amazon EBS Snapshot</h4>
<p class="noindent">Only the data referenced exclusively by a snapshot is deleted when you delete a snapshot. Deleting previous snapshots of a volume does not affect your ability to restore volumes from later snapshots. The original volume is not affected when you delete a snapshot of that volume. When you delete a volume, it will not have any impact on the snapshots made from the deleted volume. The snapshots are incremental when you create periodic snapshots of a volume for backup. You just need to retain only the most recent snapshot in order to restore the volume even though snapshots are saved incrementally. You need to retrieve all of the snapshots for your multivolume group to delete multivolume snapshots, using the tag you applied to the group when you created the snapshots. You are allowed to delete individual snapshots in the multivolume snapshots group.</p>
<h4 class="h4" id="ch9lev2sec5"><span epub:type="pagebreak" id="page_255"/>Copy an Amazon EBS Snapshot</h4>
<p class="noindent">You can copy an Amazon EBS snapshot from one AWS region to another or within the same region, and the data in transit uses Amazon S3 server-side encryption (256-bit AES). The snapshot copy receives a different snapshot ID than the original snapshot ID. You need to individually copy the snapshots when copying multivolume snapshots to another region. You need to either modify the snapshot permissions or make the snapshot public for someone from another account to be able to copy your snapshot. As long as the encryption status of a snapshot copy does not change and it happens in the same region and in the same AWS account, then the copy operation does not copy any actual data, so you will not incur any data transfer charges.</p>
<div class="siden">
<p class="imagen"><img alt="Images" class="inlinen" src="examtip.jpg"/></p>
<p class="note"><strong>EXAM TIP</strong>   When you copy a snapshot to a new region or use a new CMK to encrypt, a full copy of the data is created, which results in an additional delay and additional storage costs.</p>
</div>
<p class="indent">The following are some of the snapshot copy use cases and scenarios:</p>
<p class="bullett">• You are expanding your company to serve different geographic locations and want to launch your applications in a new AWS region.</p>
<p class="bulleta">• You plan to perform migration of your application from the US East region to US West region, where you have the majority of your customers, to enable better availability and to minimize cost.</p>
<p class="bulleta">• You need to back up your application and database, including logs, across different geographical locations at regular intervals for disaster recovery. You should be able to restore your applications using point-in-time backups stored in the secondary region to minimize data loss and recovery time.</p>
<p class="bulleta">• You need to encrypt a previously unencrypted snapshot or change the key with which the snapshot is encrypted, or you need to create a copy that you own from the shared encrypted copy in order to restore a volume from it.</p>
<p class="bulleta">• You need to copy your encrypted EBS snapshots from one AWS account to another for data retention and auditing requirements, which requires preserving data logs. A different AWS account protects you even if your main account is compromised and helps prevent accidental snapshot deletions.</p>
<div class="siden">
<p class="imagen"><img alt="Images" class="inlinen" src="examtip.jpg"/></p>
<p class="note"><strong>EXAM TIP</strong>   You need to apply user-defined tags to the new snapshot after the copy operation is complete, since the user-defined tags are not copied from the source snapshot to the new snapshot.</p>
</div>
<h4 class="h4" id="ch9lev2sec6">Amazon EBS Snapshot Lifecycle</h4>
<p class="noindent">Amazon Data Lifecycle Manager (Amazon DLM) can be used to automate the creation, retention, and deletion of snapshots taken to back up your Amazon EBS volumes. DLM allows you to protect valuable data by enforcing a regular backup schedule and retain <span epub:type="pagebreak" id="page_256"/>backups based on the duration required by auditors or internal compliance. It also saves costs by deleting outdated backups. Amazon DLM can be combined with Amazon CloudWatch events and AWS CloudTrail to provide a complete backup solution for EBS volumes at no additional cost.</p>
<h3 class="h3" id="ch9lev1sec6">Amazon EBS Elastic Volumes</h3>
<p class="noindent">Amazon EBS Elastic Volumes can be used to increase the volume size, change the volume type, or adjust the performance of your EBS volumes. You can change the configuration of Elastic Volumes without detaching the volume or restarting the instance, which enables you to continue using your application while the changes take effect.</p>
<h3 class="h3" id="ch9lev1sec7">Amazon EBS Encryption</h3>
<p class="noindent">You can create encrypted EBS volumes to meet a wide range of data-at-rest encryption requirements for regulation and audit compliance data and applications. The encryption of data in transit from the EC2 instance to Amazon EBS storage uses 256-bit Advanced Encryption Standard algorithms (AES-256) and an Amazon-managed key infrastructure. When creating encrypted volumes and encrypted volume snapshots, Amazon EBS encryption uses AWS Key Management Service (AWS KMS) master keys. A default master key is created for you automatically, and it is used for Amazon EBS encryption unless you select a customer master key (CMK). When using CMK, you can create, disable, rotate, audit the encryption keys, and define access controls to protect your data. The EBS volumes are automatically encrypted when restored from an encrypted snapshot. The volume can be encrypted on the fly when restoring from an unencrypted snapshot. You can attach encrypted volumes only to the instance types that support EBS encryption. In order to restore an EBS volume from a shared encrypted snapshot, you need to create a copy of the snapshot and then it can be restored from that copy.</p>
<p class="indent">Amazon EBS encryption uses AWS KMS CMKs when creating encrypted volumes, which means you don’t need to build, maintain, and secure your own key management infrastructure. When you create an encrypted EBS volume and attach it to a supported instance type, the data at rest inside the volume, all the data moving between the volume and the instance, all the snapshots created from the volume, all volumes created from those snapshots, and both the boot and data volumes of an EC2 instance are encrypted. Encryption operations happen on the servers where EC2 instances are hosted to ensure the security of both data at rest and data in transit between an instance and its EBS storage. Encryption is supported by all EBS volume types, and with a minimal impact on latency, you can expect similar IOPS performance on both encrypted and unencrypted volumes. Both the encrypted and unencrypted volumes can be accessed the same way. Encryption and decryption require no additional action from you or your applications and are handled transparently.</p>
<h4 class="h4" id="ch9lev2sec7">Key Management</h4>
<p class="noindent">A unique AWS-managed CMK is created automatically in each region, and you can specify a customer-managed CMK that you create to use as the default key for encryption. <span epub:type="pagebreak" id="page_257"/>The CMK associated with an existing snapshot or encrypted volume cannot be changed, but during a snapshot copy operation you can associate a different CMK so the resulting copied snapshot is encrypted using your new CMK. The industry-standard AES-256 algorithm is used to encrypt your volume and is stored on disk with your encrypted data. Any subsequent volumes created from those snapshots use the same key and are shared by snapshots of the volume.</p>
<div class="siden">
<p class="imagen"><img alt="Images" class="inlinen" src="note.jpg"/></p>
<p class="note"><strong>NOTE</strong>   You will have more flexibility and security and satisfy compliance requirements when creating your own CMK, in addition to the ability to create, rotate, and disable keys to define access controls.</p>
</div>
<h4 class="h4" id="ch9lev2sec8">Restoring and Copying Snapshots</h4>
<p class="noindent">In this section, you will see the step-by-step instructions to restore an unencrypted snapshot, restore an unencrypted snapshot with a key, copy an unencrypted snapshot, copy a snapshot with a new key, and copy an unencrypted snapshot with a key.</p>
<p class="indent">A volume restored from an unencrypted snapshot is unencrypted by default when the encryption is not enabled (which is also the default). However, by setting the encrypted parameter and the KmsKeyId parameter, the resulting volume can be encrypted. <a href="ch09.xhtml#ch9fig1">Figure 9-1</a> illustrates the process.</p>
<p class="imagef" id="ch9fig1"><img alt="images" src="f0257-01.jpg"/></p>
<p class="figcaption"><strong>Figure 9-1</strong>   Restoring an unencrypted snapshot</p>
<p class="indent">The resulting volume is encrypted using your default CMK when you leave out the KmsKeyId parameter, and you must supply a key ID to encrypt the volume to a different CMK. When you try to restore unencrypted volumes that have encryption enabled by default, then encryption is mandatory for volumes restored from unencrypted snapshots, and no other encryption parameters are required to use your default CMK. <a href="ch09.xhtml#ch9fig2">Figure 9-2</a> shows this simple default case.</p>
<p class="imagef" id="ch9fig2"><img alt="images" src="f0258-01.jpg"/></p>
<p class="figcaption"><strong>Figure 9-2</strong>   Restoring an unencrypted snapshot with a key</p>
<p class="indent">You need to provide both the Encrypted and KmsKeyId parameters when you want to encrypt the restored volume to a customer-managed CMK. A copy of an unencrypted snapshot is unencrypted by default when encryption is not enabled by default. <span epub:type="pagebreak" id="page_258"/>The resulting snapshot can be encrypted by setting the Encrypted parameter and the KmsKeyId parameter. When KmsKeyId is omitted, then the resulting snapshot is encrypted using your default CMK. In order to encrypt the volume to a different CMK, you must specify the key ID. <a href="ch09.xhtml#ch9fig3">Figure 9-3</a> illustrates the process.</p>
<p class="imagef" id="ch9fig3"><img alt="images" src="f0258-02.jpg"/></p>
<p class="figcaption"><strong>Figure 9-3</strong>   Copying an unencrypted snapshot</p>
<p class="indent">You can create an encrypted snapshot from an unencrypted snapshot by copying an unencrypted snapshot to an encrypted snapshot and then creating a volume. When you have encryption enabled by default, then encryption is mandatory for copies of unencrypted snapshots and you don’t need to provide any encryption parameters to use your default CMK. <a href="ch09.xhtml#ch9fig4">Figure 9-4</a> illustrates this default case.</p>
<p class="imagef" id="ch9fig4"><img alt="images" src="f0259-01.jpg"/></p>
<p class="figcaption"><strong>Figure 9-4</strong>   Copying an unencrypted snapshot with a key</p>
<div class="siden">
<p class="imagen"><img alt="Images" class="inlinen" src="examtip.jpg"/></p>
<p class="note"><strong>EXAM TIP</strong>   There will be an additional delay and storage costs when you copy a snapshot and encrypt it to a new CMK where a complete nonincremental copy is created always.</p>
</div>
<p class="indent"><span epub:type="pagebreak" id="page_259"/>You have the option of re-encrypting it with a different CMK when the CreateVolume action operates on an encrypted snapshot. For example, let’s say own two CMKs, CMK X and CMK Y, and the source snapshots are encrypted by CMK X, as shown in <a href="ch09.xhtml#ch9fig5">Figure 9-5</a>. The key ID of CMK Y is supplied as a parameter during the volume creation, and then the source data is automatically decrypted and then re-encrypted by CMK Y.</p>
<p class="imagef" id="ch9fig5"><img alt="images" src="f0259-02.jpg"/></p>
<p class="figcaption"><strong>Figure 9-5</strong>   Creating a volume with a new key</p>
<p class="indent">You can encrypt a snapshot during copying and apply a new CMK to an already-encrypted snapshot that you own. The restored volumes are only accessible using the new CMK. For example, let’s say you own two CMKs, CMK X and CMK Y, and the source snapshots are encrypted using CMK X, as shown in <a href="ch09.xhtml#ch9fig6">Figure 9-6</a>. During the copy, if you provide the key ID as CMK Y as a parameter, then the source data is automatically re-encrypted by CMK Y.</p>
<p class="imagef" id="ch9fig6"><img alt="images" src="f0260-01.jpg"/></p>
<p class="figcaption"><strong>Figure 9-6</strong>   Copying a snapshot with a new key</p>
<div class="siden">
<p class="imagen"><span epub:type="pagebreak" id="page_260"/><img alt="Images" class="inlinen" src="examtip.jpg"/></p>
<p class="note"><strong>EXAM TIP</strong>   When someone shares a snapshot with you, AWS recommends creating a copy of it using a different CMK that you control to protect your access to the volume because either the original CMK can be compromised or the owner might revoke the CMK for some reason.</p>
</div>
<h3 class="h3" id="ch9lev1sec8">RAID Configuration on Linux</h3>
<p class="noindent">Amazon EBS can be used to create any standard RAID configurations that you can use with a traditional bare-metal server, as long as it is supported by the operating system for your instance, since RAID is accomplished at the software level. RAID 0 stripes multiple volumes together and provides greater I/O performance than you can achieve with a single volume, and RAID 1 mirrors two volumes together to offer on-instance redundancy.</p>
<p class="imaget1"><img alt="images" src="t0260-01.jpg"/></p>
<div class="siden">
<p class="imagen"><img alt="Images" class="inlinen" src="note.jpg"/></p>
<p class="note"><strong>NOTE</strong>   Because of the parity write operations of RAID 5 and RAID 6 modes, it is not recommended for Amazon EBS, and these RAID modes consume some of the IOPS available to your volumes.</p>
</div>
<p class="indent"><span epub:type="pagebreak" id="page_261"/>You need to use EBS multivolume snapshots to create a consistent set of snapshots for your RAID array because it allows you to take data-coordinated, point-in-time snapshots. It also gives crash-consistent snapshots across multiple EBS volumes attached to an EC2 instance. The snapshots are automatically taken across multiple EBS volumes, and you do not have to stop your instance to coordinate between volumes to ensure consistency.</p>
<h3 class="h3" id="ch9lev1sec9">Amazon EBS Metrics</h3>
<p class="noindent">CloudWatch provides metrics that can be used to view, analyze, and set alarms on the operational behavior of your volumes. There are two types of monitoring data available for your Amazon EBS volumes, as mentioned in the following table.</p>
<p class="imaget1"><img alt="images" src="t0261-01.jpg"/></p>
<p class="indent">Amazon EBS emits notifications based on Amazon CloudWatch events for a different volume, encryption, and snapshot status changes. You can trigger programmatic actions in response to a change in volume, snapshot, or encryption key state by establishing rules to use CloudWatch Events as JSON objects. For example, you can trigger an AWS Lambda function when a snapshot is created to share the completed snapshot with a different account or copy it to a different region for disaster-recovery purposes.</p>
<h3 class="h3" id="ch9lev1sec10">Amazon EBS Best Practices</h3>
<p class="noindent">The following tips represent the best practices for getting optimal performance from your EBS volumes in different user scenarios:</p>
<p class="bullett">• You need to use an EBS-optimized instance because the network traffic will not be competing with the traffic between your instance and your EBS volumes, since the two types of traffic are kept separate on EBS-optimized instances.</p>
<p class="bulleta">• You need to be aware of the relationship between the maximum performance of your EBS volumes, the size and number of I/O operations, and the time it takes for each action to complete. The performance, I/O, and latency factors affect each other, and different applications are more sensitive to one factor or another.</p>
<p class="bulleta">• You need to perform initialization, also called prewarming, after restoring a new EBS volume from a snapshot because there will be a significant increase in latency when you first access each block of data. You need to access each block prior to placing the volume into production to avoid the performance hit.</p>
<p class="bulleta">• <span epub:type="pagebreak" id="page_262"/>Performance may be dropped while the snapshot is in progress for throughput-optimized HDD (st1) or cold HDD (sc1) volumes and excessive amounts of small, random I/O on the volume.</p>
<p class="bulleta">• HDD-backed volumes must maintain a queue length of four or more when performing 1MiB sequential I/O to achieve maximum consistency because the performance is affected when your application isn’t sending enough I/O requests, which is monitored by the queue length (the number of pending I/O requests from your application to your volume) and I/O size.</p>
<p class="bulleta">• For HDD volumes, AWS recommends configuring the read-ahead per-block-device setting to 1MiB for workloads that are read-heavy and access the block device through the operating system page cache. This setting can only be used when your workload consists of large, sequential I/Os; otherwise, this setting will degrade the performance.</p>
<p class="bulleta">• You should use a general-purpose SSD (gp2) volume instead of st1 or sc1 volumes when your workload consists mostly of small or random I/Os.</p>
<p class="bulleta">• Consider using a modern Linux kernel with support for indirect descriptors. Linux kernels 3.8 and above or any current-generation EC2 instance has this support.</p>
<p class="bulleta">• You can join multiple gp2, io1, st1, or sc1 volumes together in a RAID 0 configuration to use the available bandwidth for the instance types that will help delivering more I/O throughput than what a single EBS volume can deliver.</p>
<p class="bulleta">• You can analyze and view performance metrics of Amazon EBS and status checks that you can use to monitor the health of your volumes using Amazon CloudWatch.</p>
<h3 class="h3" id="ch9lev1sec11">Amazon Elastic File System</h3>
<p class="noindent">The Amazon Elastic File System (Amazon EFS) provides elastic storage capacity so as you add and remove files, it grows and shrinks automatically, so your applications have the storage when they need it by providing simple, scalable file storage for use with Amazon EC2. EFS manages the entire storage infrastructure by avoiding the complexity of patching, deploying, and maintaining complex file system configurations. The applications and tools that use the Network File System (NFS) version 4 protocol work seamlessly with Amazon EFS. An Amazon EFS file system can be accessed by multiple Amazon EC2 instances at the same time by providing a common data source for workloads and applications running on more than one instance or server.</p>
<p class="indent">You pay only for the storage used by your file system with no minimum fee, up-front costs, or setup costs for Amazon EFS, and it offers Standard and Infrequent Access (IA) storage classes. You can store frequently accessed files in the Standard storage class and long-lived, infrequently accessed files in the IA storage class, which is designed for cost-effective storage. EFS stores the data across multiple Availability Zones in an AWS region and can grow to the petabyte scale, drive high levels of throughput, and allow massively parallel access from Amazon EC2 instances to your data. Amazon EFS provides strong data consistency and file locking and uses Portable Operating System Interface (POSIX) <span epub:type="pagebreak" id="page_263"/>permissions. Amazon EFS supports both encryption in transit and encryption at rest, which can be enabled when creating an Amazon EFS file system or when you mount the file system. Amazon EFS is designed to provide two performance modes and two throughput modes:</p>
<p class="bullett">• The general-purpose performance mode is ideal for the content management systems, home directories, web-serving environments, and general file serving.</p>
<p class="bulleta">• The bursting throughput mode is ideal to scale as your file system grows.</p>
<div class="siden">
<p class="imagen"><img alt="Images" class="inlinen" src="examtip.jpg"/></p>
<p class="note"><strong>EXAM TIP</strong>   You can mount an Amazon EFS file system on instances in only one VPC at a time.</p>
</div>
<p class="indent">You need to create one or more mount targets in the Virtual Private Cloud (VPC), which provides an IP address for an NFSv4 endpoint where you can mount an Amazon EFS file system. The file system can be mounted using its Domain Name Service (DNS) name, which resolves to the IP address of the EFS mount target in the same Availability Zone of your EC2 instance. Mount targets need to be created in each Availability Zone of your AWS region. Mount targets are highly available and will fail over to other Availability Zones when the primary Availability Zone is not available. You can use EFS like any other file system after mounting the file system using the mount target. When you connect to your Amazon VPC using AWS Direct Connect, you can mount your Amazon EFS file systems on your on-premises datacenter servers as well. You can migrate your data sets to EFS or back up your on-premises data to EFS.</p>
<div class="siden">
<p class="imagen"><img alt="Images" class="inlinen" src="examtip.jpg"/></p>
<p class="note"><strong>EXAM TIP</strong>   The IP addresses and DNS for your mount targets in each AZ are static.</p>
</div>
<h3 class="h3" id="ch9lev1sec12">Amazon EFS with Amazon EC2</h3>
<p class="noindent"><a href="ch09.xhtml#ch9fig7">Figure 9-7</a> shows an example VPC accessing an Amazon EFS file system. Here, EC2 instances in the VPC have file systems mounted.</p>
<p class="imagef" id="ch9fig7"><img alt="images" src="f0264-01.jpg"/></p>
<p class="figcaption"><strong>Figure 9-7</strong>   VPC access to Amazon EFS file system</p>
<p class="indent">In the figure, the VPC has three Availability Zones and each has at least one mount target created in it. AWS recommends accessing the file system from a single mount target within the same Availability Zone. As shown in the diagram, one of the Availability Zones has two subnets but with only one mount target in one of the subnets. This setup is created as follows:</p>
<p class="bullett">• You create your Amazon VPC resources and launch your Amazon EC2 instance.</p>
<p class="bulleta">• You create your Amazon EFS file system.</p>
<p class="bulleta">• You then connect to your Amazon EC2 instance and mount the Amazon EFS file system.</p>
<h3 class="h3" id="ch9lev1sec13"><span epub:type="pagebreak" id="page_264"/>Amazon EFS with AWS Direct Connect and VPN</h3>
<p class="noindent">You can mount an Amazon EFS file system on an on-premises server to migrate on-premises data into the AWS cloud hosted in an Amazon EFS file system. You can move data from your on-premises servers into Amazon EFS and analyze it on a fleet of Amazon EC2 instances in your Amazon VPC by taking advantage of bursting. The results can be stored permanently in your file system or can be moved back to your on-premises server. AWS recommends that the on-premises server have a Linux-based operating system and have Linux kernel version 4.0 or later. Also, AWS recommends mounting an Amazon EFS file system on an on-premises server using a mount target IP address instead of a DNS name. There is no extra charge for on-premises access to your Amazon EFS file systems; however, you are charged for the AWS Direct Connect connection to your Amazon VPC. <a href="ch09.xhtml#ch9fig8">Figure 9-8</a> shows how to access an Amazon EFS file system from on-premises servers with mounted file systems.</p>
<p class="imagef" id="ch9fig8"><img alt="images" src="f0265-01.jpg"/></p>
<p class="figcaption"><strong>Figure 9-8</strong>   Connecting to Amazon EFS with AWS Direct Connect</p>
<p class="indent">Any mount target can be used from your VPC to reach the mount target’s subnet by using an AWS Direct Connect connection between your on-premises server and VPC. To access Amazon EFS from an on-premises server, add a rule to your mount <span epub:type="pagebreak" id="page_265"/>target security group to allow inbound traffic to the NFS port (2049) from your on-premises server.</p>
<p class="indent">Perform the following steps to create this setup:</p>
<p class="numbert"><strong>1.</strong> The AWS Direct Connect connection needs to be established between your on-premises datacenter and your Amazon VPC.</p>
<p class="number"><strong>2.</strong> Create your Amazon EFS file system.</p>
<p class="number"><strong>3.</strong> Mount the Amazon EFS file system on your on-premises server.</p>
<h3 class="h3" id="ch9lev1sec14">Data Consistency</h3>
<p class="noindent">Amazon EFS provides the close-to-open consistency semantics that applications expect from NFS. When an application performs synchronous write operations, they are durably stored across Availability Zones. Your application closes a file and, depending on the access pattern, Amazon EFS can provide stronger consistency guarantees than close-to-open semantics. Applications that perform synchronous data access and nonappending writes have read-after-write consistency for data access.</p>
<h3 class="h3" id="ch9lev1sec15"><span epub:type="pagebreak" id="page_266"/>Storage Classes</h3>
<p class="noindent">As mentioned, Amazon EFS offers two storage classes for your file systems:</p>
<p class="bullett">• The IA class is lower-cost storage designed for infrequently accessed, long-lived files.</p>
<p class="bulletb">• The Standard class is used to store frequently accessed files.</p>
<p class="indent">The EFS IA storage class reduces storage costs for files that aren’t accessed every day without sacrificing the high-durability, high-availability, POSIX file system access and elasticity that EFS provides. AWS recommends using EFS IA storage when you need your full data set to be readily accessible and want to automatically save on storage costs for files that are less frequently accessed to satisfy audit requirements, perform backup and recovery, or perform historical analysis.</p>
<h3 class="h3" id="ch9lev1sec16">Amazon EFS Backup</h3>
<p class="noindent">AWS offers two options for backing up your EFS file systems:</p>
<p class="bullett">• AWS Backup service</p>
<p class="bulletb">• The EFS-to-EFS backup solution</p>
<p class="indent">AWS Backup can be used as a cost-effective way to perform backups, and it is designed to simplify creating, migrating, restoring, and deleting the backups, in addition to providing improved reporting and auditing. The EFS-to-EFS backup solution includes an AWS CloudFormation template that launches, configures, and runs the AWS services required to deploy this solution in all AWS regions. You can perform file system management tasks such as creating and deleting file systems, managing tags, and managing network accessibility like creating and managing mount targets using the Amazon EFS console, the AWS Command Line Interface (AWS CLI), or programmatically. You incur charges for the amount of data in each storage class and are also charged for data access when files in IA storage are read and when files are transitioned to IA storage from Standard storage.</p>
<div class="siden">
<p class="imagen"><img alt="Images" class="inlinen" src="note.jpg"/></p>
<p class="note"><strong>NOTE</strong>   You don’t incur data access charges when using AWS Backup to back up lifecycle management–enabled EFS file systems.</p>
</div>
<h3 class="h3" id="ch9lev1sec17">Amazon EFS Encryption</h3>
<p class="noindent">Amazon EFS supports two forms of encryption for file systems: encryption in transit and encryption at rest. You may need to perform key management for encryption at rest, and Amazon EFS automatically manages the keys for encryption in transit. The data and metadata are encrypted when you create a file system that uses encryption at rest using AWS KMS and a CMK. The CMK can be managed either by AWS or by you directly. The contents of your files are encrypted at rest using the CMK that you specified when <span epub:type="pagebreak" id="page_267"/>you created your file system, and the metadata such as filenames, directory names, and directory contents are encrypted by a key that is managed by Amazon EFS. You can manage your CMKs and the contents of the encrypted file systems using AWS Identity and Access Management (IAM) policies and AWS KMS.</p>
<h3 class="h3" id="ch9lev1sec18">Lifecycle Policy</h3>
<p class="noindent">Amazon EFS Lifecycle Management automatically manages cost-effective file storage for your file systems, and it migrates files that haven’t been accessed for a specified period of time to the IA storage class. Using lifecycle policy, you have control to define when EFS transitions files to the IA storage class. A lifecycle policy applies to the entire file system, and you can specify one of four lifecycle policies: AFTER_14_DAYS, AFTER_30_DAYS, AFTER_60_DAYS, or AFTER_90_DAYS</p>
<h3 class="h3" id="ch9lev1sec19">Monitoring Amazon EFS</h3>
<p class="noindent">For any of your AWS solutions, including Amazon EFS, monitoring plays an important role in maintaining the availability, reliability, and performance by collecting the monitoring data from all of the parts of your AWS solution to make debugging a multipoint failure easier when it occurs. As soon as the monitoring is in place, you need to establish a baseline for normal Amazon EFS performance in your environment by measuring performance at various times and under different load conditions. The historical monitoring data will give you a baseline to compare against with current performance data to identify performance anomalies and normal performance patterns and to devise methods to address issues. You can monitor Amazon EFS network throughput, metadata operations, client connections, I/O for read and write, and burst credit balances of your file systems.</p>
<p class="indent">You can monitor a single metric over a period that is specified in your Amazon CloudWatch alarm, and you can perform actions based on this threshold. AWS CloudTrail logs can be monitored and stored using Amazon CloudWatch logs. You can match events using Amazon CloudWatch events and route them to one or more target functions or streams to make changes, capture state information, and take corrective action. You can share CloudTrail log files between accounts, write log processing applications, and validate that your log files have not changed after delivery, and you can monitor them in real time while sending them to CloudWatch logs.</p>
<h3 class="h3" id="ch9lev1sec20">Amazon EFS Performance</h3>
<p class="noindent">Amazon EFS file systems are distributed across storage servers to grow elastically to a petabyte scale to support massively parallel access from Amazon EC2 instances to your data. The Amazon EFS data is distributed across multiple Availability Zones, providing a high level of durability and availability and to meet the performance needs of the following use cases:</p>
<p class="bullett">• Amazon EFS provides the high throughput to compute nodes of big data applications coupled with read-after-write consistency and low-latency file operations.</p>
<p class="bulleta">• <span epub:type="pagebreak" id="page_268"/>The video editing, broadcast processing, studio production, rendering, and sound design depend on shared storage to cut the time it takes to perform and consolidate multiple file repositories into one location for all media workflow users.</p>
<p class="bulleta">• The Amazon EFS high-throughput file system is used for content management systems that store and serve information for websites, online publications, archives and access to people across an organization.</p>
<h3 class="h3" id="ch9lev1sec21">Amazon EFS Best Practices</h3>
<p class="noindent">The following are some of the recommended best practices to increase throughput and performance, to reduce latency, and to secure your EFS file system.</p>
<p class="bullett">• The distributed nature of Amazon EFS results in a small latency overhead for each file operation, so you need to increase the average I/O size to increase the overall throughput.</p>
<p class="bulleta">• You can deliver higher throughput levels on your file system when you aggregate across instances by parallelizing your application across more instances.</p>
<p class="bulleta">• Every operation goes through a round trip between the client and Amazon EFS, and enabling asynchronous writes reduces the latency because the pending write operations are buffered on the Amazon EC2 instance before they are written to Amazon EFS asynchronously.</p>
<p class="bulleta">• Amazon EFS supports the Network File System versions 4.0 and 4.1 (NFSv4) protocols when mounting your file systems on Amazon EC2 instances, and NFSv4.1 provides better performance.</p>
<p class="bulleta">• Choose instance types that have the amount of resources that your application needs when it performs a large number of read and write operations, since it needs more memory or computing capacity.</p>
<p class="bulleta">• Amazon EFS supports two forms of encryption, so choose to enable either or both types so your file system has a minimal effect on I/O latency and throughput.</p>
<p class="bulleta">• You can enable encryption of data at rest when creating an Amazon EFS file system and enable encryption of data in transit when you mount the file system by enabling Transport Layer Security (TLS).</p>
<p class="bulleta">• You need to enforce data encryption policies for Amazon EFS file systems by using Amazon CloudWatch and AWS CloudTrail to detect the creation of a file system and verify that encryption is enabled.</p>
<h3 class="h3" id="ch9lev1sec22">Amazon S3 Glacier</h3>
<p class="noindent">Amazon Simple Storage Service Glacier (S3 Glacier) is a storage service optimized for infrequently used cold data, and it is a very low-cost storage service that provides durable storage with security features for data archiving and backup. Glacier enables customers to store their data cost-effectively for months, years, or even decades. You can offload the <span epub:type="pagebreak" id="page_269"/>administrative burdens like operations and scaling of storage to AWS, which saves you from hardware provisioning, capacity planning, data replication, time-consuming hardware migrations, or hardware failure detection and recovery. In Amazon S3 Glacier, a vault is a container for storing any object like photos, videos, or documents. You can use Amazon S3 lifecycle configuration to transition objects to the Amazon S3 Glacier storage class for archival. Amazon S3 uses Glacier internally for durable storage at a lower cost. Objects remain Amazon S3 objects even when stored in Glacier, but you cannot access them directly through Glacier. Glacier is a REST-based web service, and its data model includes vaults, archives, jobs, and notification configuration resources.</p>
<div class="siden">
<p class="imagen"><img alt="Images" class="inlinen" src="examtip.jpg"/></p>
<p class="note"><strong>EXAM TIP</strong>   You can use the Glacier console to create and delete vaults, but all other interactions with Glacier require that you use the AWS CLI or write code.</p>
</div>
<h4 class="h4" id="ch9lev2sec9">Vault</h4>
<p class="noindent">In Glacier, a vault is a container for storing archives. You need to specify a name and AWS region when you create a vault, and each vault resource will have a unique address. The general form is:</p>
<p class="imagep"><img alt="images" src="p0269-01.jpg"/></p>
<p class="indent">For example, suppose that you create a vault (samplevault) in the US East (N. Virginia) Region, and it can then be addressed as shown:</p>
<p class="imagep"><img alt="images" src="p0269-02.jpg"/></p>
<p class="indent">In the URI, <a href="http://glacier.us-east-1.amazonaws.com">glacier.us-east-1.amazonaws.com</a> identifies the US East (N. Virginia) region. The AWS account ID vault is 444466668888, and “vaults” refers to the collection of vaults owned by that AWS account. The specific vault in the vaults collection is “samplevault.” Vaults can be created in any supported AWS region, and the vault name must be unique in a particular region, although you can create vaults with the same name in different regions. Based on business or application needs, an unlimited number of archives can be stored in a vault or multiple vaults. Many vault operations, such as creating a vault or listing the vaults, are region specific and you cannot perform those operations from a different region.</p>
<h4 class="h4" id="ch9lev2sec10">Archives</h4>
<p class="noindent">An archive is a base unit of storage in Glacier, which can be any data item such as a photo, video, or document. Glacier assigns the archive an ID and description, which you can specify only during the upload of an archive. Each archive has a unique address. The general form is as follows:</p>
<p class="imagep"><img alt="images" src="p0269-03.jpg"/></p>
<p class="indent"><span epub:type="pagebreak" id="page_270"/>Following is an example URI of an archive stored in the samplevault vault in the US East (N. Virginia) region:</p>
<p class="imagep"><img alt="images" src="p0270-01.jpg"/></p>
<h4 class="h4" id="ch9lev2sec11">Jobs</h4>
<p class="noindent">You can use Glacier jobs to retrieve an archive, perform a select query on an archive, or get an inventory of a vault. When performing a query on an archive, you initiate a job providing a SQL query and a list of Glacier archive objects. You can use Glacier Select to run the query in place and can write the output results to Amazon S3. Retrieving an archive and vault inventory (list of archives) are asynchronous operations in Glacier in which you first initiate a job and then download the job output after Glacier completes it. You need to provide a vault name to initiate a vault inventory job. Select and archive retrieval jobs require the vault name, archive ID, and job description to help identify the jobs. You can run multiple jobs in a vault at any point in time, and each job is uniquely identified as follows:</p>
<p class="imagep"><img alt="images" src="p0270-02.jpg"/></p>
<p class="indent">The following is an example of a job associated with the samplevault vault:</p>
<p class="imagep"><img alt="images" src="p0270-03.jpg"/></p>
<p class="indent">Glacier maintains information such as job type, description, creation date, completion date, and job status for each job. The information about a particular job or list of jobs can be obtained from a vault. The list of jobs that Glacier returns includes all the in-progress and recently finished jobs.</p>
<div class="siden">
<p class="imagen"><img alt="Images" class="inlinen" src="examtip.jpg"/></p>
<p class="note"><strong>EXAM TIP</strong>   Glacier is a cold storage system that you can use as a data archival solution at a very low cost. If you need a storage system that requires real-time data retrieval, you should use Amazon S3.</p>
</div>
<h4 class="h4" id="ch9lev2sec12">Notification Configuration</h4>
<p class="noindent">Glacier uses the Amazon Simple Notification Service (Amazon SNS) to send a notification when a job is complete. An SNS topic can be specified for each vault in the notification configuration, and it is stored as a JSON document. The following is an example vault notification configuration:</p>
<p class="imagep"><img alt="images" src="p0270-04.jpg"/></p>
<p class="indent"><span epub:type="pagebreak" id="page_271"/>The vault notification configuration resource is uniquely identified by a URI in the form:</p>
<p class="imagep"><img alt="images" src="p0271-01.jpg"/></p>
<p class="indent">You can use notification configurations for set, get, and delete operations.</p>
<h3 class="h3" id="ch9lev1sec23">Glacier Operations</h3>
<p class="noindent">The following asynchronous operations can be used to work with vaults and archives:</p>
<p class="bullett">• Retrieving an archive</p>
<p class="bulletb">• Retrieving a vault inventory (list of archives)</p>
<p class="indent">You need to first initiate a job and then download the job output. A Glacier job can be initiated to perform a select query on an archive, retrieve an archive, or get an inventory of a vault. The following are the types of Glacier jobs:</p>
<p class="bullett">• <strong>Select</strong>   Performs a select query on any archive.</p>
<p class="bulleta">• <strong>archive-retrieval</strong>   Retrieves an item from an archive.</p>
<p class="bulletb">• <strong>inventory-retrieval</strong>   Retrieves the inventory from a vault.</p>
<p class="indent">Vaults can be created in a specific AWS region and send your requests to a region-specific endpoint.</p>
<div class="siden">
<p class="imagen"><img alt="Images" class="inlinen" src="examtip.jpg"/></p>
<p class="note"><strong>EXAM TIP</strong>   Glacier provides a management console, and you can use it to create and delete vaults. However, you need to use the AWS CLI or write the code to make requests, using either the REST API directly or by using the AWS SDKs for all other interactions with Glacier.</p>
</div>
<h4 class="h4" id="ch9lev2sec13">Creating and Deleting Vaults</h4>
<p class="noindent">You can create up to 1,000 vaults per region, and you can delete a vault only if there are no archives in it and no writes to the vault since the last inventory.</p>
<div class="siden">
<p class="imagen"><img alt="Images" class="inlinen" src="caution.jpg"/></p>
<p class="note"><strong>CAUTION</strong>   Glacier prepares an inventory for each vault periodically every 24 hours, so the inventory might not reflect the latest information. Glacier ensures the vault is empty by checking whether there have been any write operations from the time of the last vault inventory.</p>
</div>
<p class="indent">You can use application programming interface (API) calls to retrieve vault information such as the vault creation date, number of archives in the vault, and the total size of all the archives in the Glacier vault in a specific region in your account. You can download a vault inventory that provides archive information such as the archive ID, creation <span epub:type="pagebreak" id="page_272"/>date, and size. Glacier updates the vault inventory approximately once a day, and the vault inventory must exist for you to be able to download it. You need to initiate a job (POST jobs) to retrieve anything from Glacier, and then you can download the output (GET output) when the job completes using Glacier notifications.</p>
<div class="siden">
<p class="imagen"><img alt="Images" class="inlinen" src="tip.jpg"/></p>
<p class="note"><strong>TIP</strong>   If you have a restricted retrieval policy, your retrieval job request may fail with a PolicyEnforcedException exception.</p>
</div>
<p class="indent">The status of the job can be determined by job completion notification, or you can explicitly request a describe job operation (Describe Job (GET JobID)). As mentioned, an Amazon S3 Glacier vault can be deleted only when there are no archives in the vault since the last inventory and no writes to the vault from the time of the last inventory.</p>
<h4 class="h4" id="ch9lev2sec14">Archive Operations</h4>
<p class="noindent">You can use Glacier to upload, download, and delete archives. Archives can be uploaded in a single operation, or you can upload them in parts using API calls. When you initiate a job to download a specific archive, Glacier prepares the archive for download; after the job completes, you can download your archive data, which is an asynchronous operation. You can use API calls to delete archives. You cannot update the archive content or its description after the upload. The only way to update is by deleting the archive and uploading another archive. Glacier returns a unique archive ID each time you upload an archive.</p>
<p class="indent">You can make an Expedited, Standard, or Bulk retrieval by setting the Tier parameter in the Initiate Job (POST jobs) REST API request, or in the AWS CLI or AWS SDKs Choose from the following options when initiating a job to retrieve an archive based on your access time and cost requirements:</p>
<p class="bullett">• <strong>Expedited</strong>   Allows you to quickly access your data typically within one to five minutes. The retrieval capacity is based on the provisioned capacity for Expedited retrievals.</p>
<p class="bulleta">• <strong>Standard</strong>   Allows you to access any of your archives within 3 to 5 hours, which is the default option for retrieval requests that do not specify any retrieval option.</p>
<p class="bulletb">• <strong>Bulk</strong>   Allows you to retrieve large amounts of data inexpensively in a day and takes 5 to12 hours to complete.</p>
<p class="indent">You need to have Provisioned capacity to ensure the retrieval capacity for expedited retrievals is available when you need it, and each unit of capacity provides up to 150 MB/s of retrieval throughput. You can specify a range or portion of the archive to retrieve when you retrieve an archive from Glacier; the default is to retrieve the entire archive. You can use Glacier Select to perform filtering operations using simple Structured Query Language (SQL) statements directly on your data in Glacier. Glacier Select runs the SQL query in place and writes the output results to Amazon S3.</p>
<h4 class="h4" id="ch9lev2sec15"><span epub:type="pagebreak" id="page_273"/>Data Retrieval Policy</h4>
<p class="noindent">There are three types of Glacier data retrieval policies: No Retrieval Limit, Free Tier Only, and Max Retrieval Rate. No Retrieval Limit is the default policy, and when used, no retrieval limit is set and all valid data retrieval requests are accepted. The Free Tier Only policy can be used to keep your retrievals within your daily Free Tier allowance and not incur any additional cost. The Max Retrieval Rate policy enables the peak retrieval rate from all retrieval jobs across your account in a region, and it will not exceed the bytes-per-hour limit you set. The data retrieval policy can be viewed by using the Glacier REST API or by using the AWS software development kits (SDKs).</p>
<h3 class="h3" id="ch9lev1sec24">Vault Lock</h3>
<p class="noindent">The compliance controls can be enforced by using a Glacier Vault Lock policy, such as write once read many (WORM), and lock the policy against future edits. The policy can no longer be changed once it is locked. A vault access policy is different from a vault lock policy, and both govern access controls to your vault. The vault lock policy is used to prevent future changes by providing strong enforcement for your compliance controls. The vault lock policy is used to deploy regulatory and compliance controls, and the vault access policy is used to implement access controls that are not temporary or compliance related and are subject to frequent modification. Both policies can be used together, for example, to implement time-based data retention rules in the vault lock policy to deny deletes and grant read access to third parties or your business partners. The lock is initiated when attaching a vault lock policy to your vault that sets the lock to an in-progress state and returns a lock ID, and you can validate your vault lock policy within 24 hours before the lock ID expires. The lock ID is used to complete the lock process. You can abort the lock and restart from the beginning if the vault lock policy doesn’t work as expected.</p>
<h3 class="h3" id="ch9lev1sec25">Data Protection</h3>
<p class="noindent">Amazon S3 Glacier delivers 99.999999999 percent durability and provides comprehensive security and compliance capabilities to help you meet stringent regulatory requirements by synchronously storing your data across multiple Availability Zones. You can protect your data using Secure Sockets Layer (SSL) or client-side encryption while in transit while traveling to and from Amazon S3 Glacier and at rest when it is stored in AWS datacenters. You can use Amazon S3 lifecycle configuration on an Amazon S3 bucket to transition objects to the Amazon S3 Glacier storage class for archival. The data in transit between Amazon S3 and Glacier via lifecycle policies is encrypted using SSL, and the data stored at rest in Glacier is automatically encrypted using either server-side encryption or client-side encryption. AWS recommends TLS 1.2 or later. You must sign requests using an access key ID and a secret access key, or use the AWS Security Token Service (AWS STS) to generate temporary security credentials to sign requests.</p>
<h3 class="h3" id="ch9lev1sec26"><span epub:type="pagebreak" id="page_274"/>Logging and Monitoring</h3>
<p class="noindent">Monitoring is an important part of maintaining the reliability, availability, and performance of Amazon S3 Glacier, and you should collect monitoring data to easily identify and debug the source of a failure when it occurs. Amazon CloudWatch alarms can be used to watch a single metric over a time period that you specify. A notification is sent to an Amazon SNS topic or AWS Auto Scaling policy when the metric exceeds a given threshold. CloudTrail captures the API calls from the Glacier console and code calls to the Glacier APIs as events. You can use Trusted Advisor to get the best practices learned from many AWS customers, and it inspects your AWS environment and makes appropriate recommendations to improve system availability and performance, to save money, and to help close security gaps. The third-party auditors, as part of multiple AWS compliance programs, access the security and compliance of Amazon S3 Glacier, including</p>
<p class="bullett">• Health Insurance Portability and Accountability Act (HIPAA)</p>
<p class="bulleta">• System and Organization Controls (SOC)</p>
<p class="bulleta">• Federal Risk and Authorization Management Program (FedRAMP)</p>
<p class="bulletb">• Payment Card Industry (PCI) Data Security Standard</p>
<p class="indent">The third-party audit reports are available in AWS Artifact for download. You can use AWS Config to assess how well your resource configurations comply with industry guidelines, internal practices, and regulations. You can use AWS Security Hub to get a comprehensive view of your security state within AWS, which can help you comply with the security, industry standards, and best practices.</p>
<h3 class="h3" id="ch9lev1sec27">Chapter Review</h3>
<p class="noindent">This chapter explained Amazon Elastic Block Store, Amazon Elastic File System, and Amazon S3 Glacier in detail. Amazon EBS provides block level storage volumes, which behave like raw, unformatted block devices and are used for EC2 instances, and you pay only for what you use. You can create an Amazon EBS volume in the same Availability Zone where you want to attach it to an EC2 instance as an encrypted EBS volume, but it can only be attached to selected instance types. An Amazon EBS volume can be detached from an instance explicitly or when you terminate the instance. You need to unmount the volume from the instance if the instance is running. Before you can detach the volume, you need to stop the instance if an EBS volume is the root device of an instance. You can delete an Amazon EBS volume if you no longer need it. After deleting the volume, its data is not available, nor can it be attached to any instance. You can create backups of your Amazon EBS volumes using snapshots, which are redundantly stored in multiple Availability Zones in Amazon S3. You can take snapshots even when the volume does not need to be attached to a running instance. Multivolume snapshots can be taken across multiple EBS volumes attached to an EC2 instance, which allows you to take exact point-in-time, crash-consistent, and data-coordinated snapshots. You can copy Amazon EBS snapshots from one AWS region to another or within the same <span epub:type="pagebreak" id="page_275"/>region, and the data in transit uses Amazon S3 server-side encryption (256-bit AES). Amazon DLM can be used to automate the creation, retention, and deletion of snapshots taken to back up your Amazon EBS volumes. You can analyze and view performance metrics for Amazon EBS and status checks that you can use to monitor the health of your volumes using Amazon CloudWatch.</p>
<p class="indent">Amazon EFS provides elastic storage capacity, so as you add and remove files, it grows and shrinks automatically—thus, your applications have the storage when they need it by providing simple, scalable file storage for use with Amazon EC2. You can mount an Amazon EFS file system on an on-premises server to migrate on-premises data into the AWS cloud. Amazon EFS provides close-to-open consistency semantics that applications expect from NFS. When an application performs a synchronous write operation, these operations are durably stored across Availability Zones. Amazon EFS offers two storage classes for your file systems. The IA class is a lower-cost storage system designed for infrequently accessed, long-lived files, and the Standard class is used to store frequently accessed files. Amazon EFS supports two forms of encryption for file systems: encryption in transit and encryption at rest. You may need to perform key management for encryption at rest, and Amazon EFS automatically manages the keys for encryption in transit. Amazon EFS file systems are distributed across storage servers to grow elastically to the petabyte scale to allow massively parallel access from Amazon EC2 instances to your data.</p>
<p class="indent">Amazon Simple Storage Service Glacier is a storage service optimized for infrequently used cold data, and it is a very low-cost storage service that provides durable storage with security features for data archiving and backup. Glacier enables customers to store their data cost-effectively for months, years, or even decades. In Glacier, a vault is a container for storing archives. You need to specify a name and AWS region when you create the vault, and each vault resource will have a unique address. An archive is a base unit of storage in Glacier, which can be any data such as a photo, video, or document. You can use Glacier jobs to retrieve an archive, perform a select query on an archive, or get an inventory of a vault. When performing a query on an archive, you initiate a job providing a SQL query and list of Glacier archive objects. Glacier uses Amazon SNS to send a notification when a job is complete. The SNS topic can be specified for each vault in the notification configuration, and it is stored as a JSON document. There are three types of Glacier data retrieval policies: No Retrieval Limit, Free Tier Only, and Max Retrieval Rate. The compliance controls can be enforced by using a Glacier Vault Lock policy such as WORM and lock the policy from future edits. Monitoring is an important part of maintaining the reliability, availability, and performance of Amazon S3 Glacier, and you should collect monitoring data to easily identify and debug the source of a failure when it occurs.</p>
<h4 class="h4" id="ch9lev2sec16">Exercises</h4>
<p class="noindent">The following exercises will help you practice using Amazon EBS, EFS, and Glacier to perform various tasks.</p>
<p class="indent">You need to create an AWS account, as explained earlier, to perform the following exercises. You can use the Free Tier when launching AWS resources, but make sure to terminate them when you are done.</p>
<h5 class="h5"><span epub:type="pagebreak" id="page_276"/>Exercise 9-1: Create a New EBS Volume Using the Console</h5>
<p class="noindent">In this exercise, the step-by-step instructions will help you create a new EBS volume using the AWS Management Console.</p>
<p class="numbert"><strong>1.</strong> Log in to your AWS account and open the Amazon EC2 console at <a href="https://console.aws.amazon.com/ec2/">https://console.aws.amazon.com/ec2/</a>.</p>
<p class="number"><strong>2.</strong> From the top right side of the navigation bar, select the region where you would like to create your volume.</p>
<p class="number"><strong>3.</strong> In the navigation pane on the left, choose Amazon Elastic Block Store and then Volumes.</p>
<p class="number"><strong>4.</strong> Choose Create Volume From Here.</p>
<p class="number"><strong>5.</strong> Choose a volume type.</p>
<p class="number"><strong>6.</strong> For the Size (GiB) field, type <strong>5</strong> GiB for the volume</p>
<p class="number"><strong>7.</strong> Choose the Provisioned IOPS SSD volume, and type <strong>250</strong> for the maximum number of IOPS.</p>
<p class="number"><strong>8.</strong> Choose us-east-1a for the Availability Zone.</p>
<p class="number"><strong>9.</strong> Select the Encrypted box and choose aws/ebs (the default) for the Master Key.</p>
<p class="number1"><strong>10.</strong> Choose Tag, type <strong>key</strong> for the Name and <strong>My first EBS volume</strong> for the Volume.</p>
<p class="number1"><strong>11.</strong> Choose Create Volume.</p>
<h5 class="h5">Exercise 9-2: Create an EBS Volume from a Snapshot Using the Console</h5>
<p class="number"><strong>1.</strong> In this exercise, the step-by-step instructions will assist you in creating an EBS volume from the backup snapshot using the AWS Management Console. Log in to your AWS account and open the Amazon EC2 console at <a href="https://console.aws.amazon.com/ec2/">https://console.aws.amazon.com/ec2/</a>.</p>
<p class="number"><strong>2.</strong> From the top right side of navigation bar, select the region where you would like to create your volume.</p>
<p class="number"><strong>3.</strong> In the left navigation pane, choose Elastic Block Store and then Volumes.</p>
<p class="number"><strong>4.</strong> Choose Create Volume.</p>
<p class="number"><strong>5.</strong> Choose a volume type.</p>
<p class="number"><strong>6.</strong> Start typing the ID or description of the snapshot from which you are restoring the volume, and choose it from the dropdown list of suggestions.</p>
<p class="number"><strong>7.</strong> Select Encrypt This Volume to change the encryption of your new volume.</p>
<p class="number"><strong>8.</strong> For the Size (GiB) field, type <strong>5</strong>.</p>
<p class="number"><strong>9.</strong> Choose us-east-1a for the Availability Zone.</p>
<p class="number1"><strong>10.</strong> Choose Add Tags and type <strong>Name</strong> for the Key and <strong>My second EBS volume</strong> for the Value.</p>
<p class="number1"><strong>11.</strong> Choose Create Volume.</p>
<h5 class="h5"><span epub:type="pagebreak" id="page_277"/>Exercise 9-3: Attach an EBS Volume to an Instance Using the Console</h5>
<p class="noindent">In this exercise, the step-by-step instructions will assist you in attaching the new EBS volume to your existing Amazon EC2 instance using the AWS Management Console.</p>
<p class="numbert"><strong>1.</strong> Log in to your AWS account and open the Amazon EC2 console at <a href="https://console.aws.amazon.com/ec2/">https://console.aws.amazon.com/ec2/</a>.</p>
<p class="number"><strong>2.</strong> From the top right side of navigation bar, select the region where you would like to create your volume.</p>
<p class="number"><strong>3.</strong> In the left navigation pane, choose Elastic Block Store and then Volumes.</p>
<p class="number"><strong>4.</strong> Select a volume from the list of available volumes and choose Actions and then Attach Volume.</p>
<p class="number"><strong>5.</strong> Start typing the name or ID of the instance and select it from the dropdown list.</p>
<p class="number"><strong>6.</strong> For Device, type <strong>/dev/sdf1</strong>.</p>
<p class="number"><strong>7.</strong> Choose Attach.</p>
<p class="number"><strong>8.</strong> Now you can connect to your instance and mount the volume.</p>
<h5 class="h5">Exercise 9-4: Detach an EBS Volume Using the Console</h5>
<p class="noindent">In this exercise, the step-by-step instructions will help you detach the EBS volume from your existing Amazon EC2 instance using the AWS Management Console.</p>
<p class="numbert"><strong>1.</strong> You can use the following command to unmount the /dev/sdf1 device:</p>
<p class="numberc"><img alt="images" src="p0277-01.jpg"/></p>
<p class="number"><strong>2.</strong> Log in to your AWS account and open the Amazon EC2 console at <a href="https://console.aws.amazon.com/ec2/">https://console.aws.amazon.com/ec2/</a>.</p>
<p class="number"><strong>3.</strong> From the top right side of navigation bar, select the region where you would like to create your volume.</p>
<p class="number"><strong>4.</strong> From the left navigation pane, choose Elastic Block Store and then Volumes.</p>
<p class="number"><strong>5.</strong> Select a volume you want to detach and choose Actions, and then Detach Volume.</p>
<p class="number"><strong>6.</strong> In the confirmation dialog box, choose Yes.</p>
<h5 class="h5">Exercise 9-5: Delete an EBS Volume Using the Console</h5>
<p class="noindent">In this exercise, the step-by-step instructions will assist you in deleting your EBS volume using the AWS Management Console.</p>
<p class="numbert"><strong>1.</strong> Log in to your AWS account and open the Amazon EC2 console at <a href="https://console.aws.amazon.com/ec2/">https://console.aws.amazon.com/ec2/</a>.</p>
<p class="number"><strong>2.</strong> From the top right side of navigation bar, select the region where you would like to delete your volume.</p>
<p class="number"><strong>3.</strong> In the left navigation pane, choose Elastic Block Store and then Volumes.</p>
<p class="number"><strong>4.</strong> <span epub:type="pagebreak" id="page_278"/>Select a volume from the list of available volumes and choose Actions and then Delete Volume.</p>
<p class="number"><strong>5.</strong> In the confirmation dialog box, choose Yes, Delete.</p>
<h5 class="h5">Exercise 9-6: Create a Snapshot Using the Console</h5>
<p class="noindent">In this exercise, the step-by-step instructions will show you how to take an EBS volume backup snapshot using the AWS Management Console.</p>
<p class="numbert"><strong>1.</strong> Log in to your AWS account and open the Amazon EC2 console at <a href="https://console.aws.amazon.com/ec2/">https://console.aws.amazon.com/ec2/</a>.</p>
<p class="number"><strong>2.</strong> From the top right side of navigation bar, select the region where you would like to create your snapshot.</p>
<p class="number"><strong>3.</strong> In the left navigation pane, choose Elastic Block Store and then Snapshots.</p>
<p class="number"><strong>4.</strong> Choose Create Snapshot.</p>
<p class="number"><strong>5.</strong> From Select Resource Type, choose Volume and enter a description of the snapshot.</p>
<p class="number"><strong>6.</strong> Choose Add Tags and type <strong>Name</strong> for the key and <strong>My EBS snapshot</strong> for the value.</p>
<p class="number"><strong>7.</strong> Choose Create Snapshot.</p>
<h5 class="h5">Exercise 9-7: Delete a Snapshot Using the Console</h5>
<p class="noindent">In this exercise, the step-by-step instructions will help you delete your EBS volume snapshot using the AWS Management Console.</p>
<p class="numbert"><strong>1.</strong> Log in to your AWS account and open the Amazon EC2 console at <a href="https://console.aws.amazon.com/ec2/">https://console.aws.amazon.com/ec2/</a>.</p>
<p class="number"><strong>2.</strong> From the top right side of navigation bar, select the region where you would like to delete your volume.</p>
<p class="number"><strong>3.</strong> From the left navigation pane, choose Elastic Block Store and then Snapshots.</p>
<p class="number"><strong>4.</strong> Select a snapshot from the list of available snapshots and then choose Delete from the Actions list.</p>
<p class="number"><strong>5.</strong> Choose Yes, Delete.</p>
<h4 class="h4" id="ch9lev2sec17">Questions</h4>
<p class="noindent">The following questions will help you gauge your understanding of the material in this chapter. Read all the answers carefully because there might be more than one correct answer. Choose the best response for each question.</p>
<p class="numbert"><strong><a href="ch09.xhtml#rch09qa1" id="ch09qa1">1.</a></strong> Which of the following Amazon EBS volume types provide single-digit milliseconds latency between EC2 instances and EBS?</p>
<p class="alphau"><strong>A.</strong> Provisioned IOPS SSD (io1)</p>
<p class="alphau"><strong>B.</strong> General-Purpose SSD (gp2)</p>
<p class="alphau"><strong>C.</strong> Throughput-Optimized HDD (st1)</p>
<p class="alphau"><strong>D.</strong> Cold HDD (sc1)</p>
<p class="number"><strong><a href="ch09.xhtml#rch09qa2" id="ch09qa2">2.</a></strong> <span epub:type="pagebreak" id="page_279"/>You need to keep EBS volumes intact even if the EC2 instance where your EBS volume is attached is terminated. How can you prevent the deletion of the EBS volume during EC2 instance termination?</p>
<p class="alphau"><strong>A.</strong> Change the DeleteOnTermination flag to false</p>
<p class="alphau"><strong>B.</strong> Change the RemoveOnTermination flag to false</p>
<p class="alphau"><strong>C.</strong> Change the DeleteOnTermination flag to true</p>
<p class="alphau"><strong>D.</strong> Change the TerminateOnDeletion flag to false</p>
<p class="number"><strong><a href="ch09.xhtml#rch09qa3" id="ch09qa3">3.</a></strong> You are working with the development team to migrate the application and database to the cloud. The application team needs a secure encrypted database storage option to migrate the database to an EC2 instance. What AWS storage option would you recommend?</p>
<p class="alphau"><strong>A.</strong> Amazon S3 with client-side encryption</p>
<p class="alphau"><strong>B.</strong> Amazon EBS with encryption</p>
<p class="alphau"><strong>C.</strong> Amazon EFS</p>
<p class="alphau"><strong>D.</strong> AWS Snowball</p>
<p class="number"><strong><a href="ch09.xhtml#rch09qa4" id="ch09qa4">4.</a></strong> Your DevOps team stopped all their sandbox Amazon EC2 instances to save costs, but they are still incurring charges for Amazon EBS storage. They want to know how to stop EBS charges accruing for stopped instances that they are not using. Which statement about EBS volumes is true?</p>
<p class="alphau"><strong>A.</strong> The DevOps team will be charged for the EBS volume and instance only when the instance is running.</p>
<p class="alphau"><strong>B.</strong> The DevOps team will be charged for the EBS volume even if the instance is stopped since it’s measured in gigabyte-months.</p>
<p class="alphau"><strong>C.</strong> The DevOps team will be charged only for the instance’s running cost.</p>
<p class="alphau"><strong>D.</strong> The DevOps team will not be charged for the EBS volume if the instance is stopped.</p>
<p class="number"><strong><a href="ch09.xhtml#rch09qa5" id="ch09qa5">5.</a></strong> Your performance testing team is complaining about inadequate storage, and they want an additional EBS volume attached to instance-B. The DevOps team tries to attach an underutilized volume, which is attached to another running instance-A in the same Availability Zone to instance-B but is getting an error. How can the underutilized EBS volume attached to a running instance (instance-A) be attached to a new running instance (instance-B)?</p>
<p class="alphau"><strong>A.</strong> You need to terminate the instance-A, and only then it can be attached to the instance-B.</p>
<p class="alphau"><strong>B.</strong> You can attach the volume as read only to instance-B.</p>
<p class="alphau"><strong>C.</strong> You need to detach the volume from instance-A and then attach it to instance-B.</p>
<p class="alphau"><strong>D.</strong> You don’t need to detach from instance-A. You can just select the volume and attach it to the instance-B, because mapping will be done internally.</p>
<p class="number"><strong><a href="ch09.xhtml#rch09qa6" id="ch09qa6">6.</a></strong> <span epub:type="pagebreak" id="page_280"/>An EC2 instance uses an EBS-backed root volume and an instance store (i.e., ephemeral store) volume for temporary processes. Can you stop this EC2 instance and, if so, what happens to the data on any ephemeral store volumes when it is started?</p>
<p class="alphau"><strong>A.</strong> The instance can’t be stopped, and the ephemeral data is automatically saved in an EBS volume.</p>
<p class="alphau"><strong>B.</strong> The instance can be stopped, and the ephemeral data is unavailable until the instance is restarted.</p>
<p class="alphau"><strong>C.</strong> The instance can be stopped, and the ephemeral data will be deleted and will no longer be accessible.</p>
<p class="alphau"><strong>D.</strong> The instance can’t be stopped, and the ephemeral data is automatically saved as an EBS snapshot.</p>
<p class="number"><strong><a href="ch09.xhtml#rch09qa7" id="ch09qa7">7.</a></strong> Your application team is looking for file storage for use with Amazon EC2 that provides strong consistency, file locking, and is concurrently accessible by thousands of other Amazon EC2 instances. Which cloud storage service supports the required storage workload?</p>
<p class="alphau"><strong>A.</strong> Amazon EBS, which is block-level storage and can be used with Amazon EC2</p>
<p class="alphau"><strong>B.</strong> Amazon S3, which provides object storage and can be accessed anywhere</p>
<p class="alphau"><strong>C.</strong> Amazon EFS, which provides a file system interface that can be used with Amazon EC2</p>
<p class="alphau"><strong>D.</strong> Amazon S3 Glacier, which provides extremely low-cost storage for data archival and backup</p>
<p class="number"><strong><a href="ch09.xhtml#rch09qa8" id="ch09qa8">8.</a></strong> Your management team has asked to you to find a strategy that saves costs on backing up files that are not accessed every day. Your cloud architect informed you that the EFS IA storage class could save up to 85 percent of the EFS Standard class. What can you do to move infrequently accessed files to another storage class and save costs?</p>
<p class="alphau"><strong>A.</strong> Enable the S3 Lifecycle Management policy to move the infrequently accessed files to Amazon S3 Glacier.</p>
<p class="alphau"><strong>B.</strong> Enable EFS Lifecycle Management and choose an age-off policy to move the files to EFS IA, which automatically moves your data to the EFS IA storage class based on the lifecycle policy.</p>
<p class="alphau"><strong>C.</strong> Set up a daily job to move infrequently accessed files to the EFS IA storage class.</p>
<p class="alphau"><strong>D.</strong> Delete the old files that are no longer required and move the infrequently accessed files to EBS storage to save costs.</p>
<p class="number"><strong><a href="ch09.xhtml#rch09qa9" id="ch09qa9">9.</a></strong> <span epub:type="pagebreak" id="page_281"/>As part of cost optimization, you have identified huge chunks of old files in tapes and other old storage media that may not be required in the future unless there is a compliance issue. You want to save those files in a cost-effective way. What is the best possible solution that saves costs for the archival storage?</p>
<p class="alphau"><strong>A.</strong> Amazon EFS</p>
<p class="alphau"><strong>B.</strong> Amazon S3</p>
<p class="alphau"><strong>C.</strong> Amazon EBS</p>
<p class="alphau"><strong>D.</strong> Amazon S3 Glacier</p>
<p class="number1"><strong><a href="ch09.xhtml#rch09qa10" id="ch09qa10">10.</a></strong> You are planning an archival strategy for your company to store all infrequently accessed data in Amazon S3 Glacier. You want to know the maximum limit, minimum limit, and size of an individual archival file to plan this efficiently. Which of the following statements is true?</p>
<p class="alphau"><strong>A.</strong> Maximum 5 terabytes, no minimum, individual archive 1 byte to 4 terabytes.</p>
<p class="alphau"><strong>B.</strong> No maximum, minimum 5 terabytes, individual archive 1 byte to 14 terabytes</p>
<p class="alphau"><strong>C.</strong> No maximum, no minimum, individual archive 1 byte to 40 terabytes</p>
<p class="alphau"><strong>D.</strong> Maximum 999 terabytes, minimum 1 byte, individual archive 1 byte to 44 terabytes</p>
<h4 class="h4" id="ch9lev2sec18">Answers</h4>
<p class="number"><strong><a href="ch09.xhtml#ch09qa1" id="rch09qa1">1.</a> A.</strong> The Provisioned IOPS SSD (io1) helps you achieve an average of single-digit millisecond latency between EC2 instances and EBS.</p>
<p class="number"><strong><a href="ch09.xhtml#ch09qa2" id="rch09qa2">2.</a> A.</strong> By changing the DeleteOnTermination flag to false, you can prevent the deletion of the EBS volume during EC2 instance termination.</p>
<p class="number"><strong><a href="ch09.xhtml#ch09qa3" id="rch09qa3">3.</a> B.</strong> Amazon EBS with encryption will help the application team secure database storage in the cloud.</p>
<p class="number"><strong><a href="ch09.xhtml#ch09qa4" id="rch09qa4">4.</a> B.</strong> The DevOps team will be charged for the EBS volume even if the instance is stopped, since it’s measured in gigabyte-months.</p>
<p class="number"><strong><a href="ch09.xhtml#ch09qa5" id="rch09qa5">5.</a> C.</strong> You need to detach the volume from instance-A and then attach it to instance-B.</p>
<p class="number"><strong><a href="ch09.xhtml#ch09qa6" id="rch09qa6">6.</a> C.</strong> The instance can be stopped, and the ephemeral data will be deleted and will no longer be accessible.</p>
<p class="number"><strong><a href="ch09.xhtml#ch09qa7" id="rch09qa7">7.</a> C.</strong> Use Amazon EFS, which provides a file system interface that can be used with Amazon EC2.</p>
<p class="number"><strong><a href="ch09.xhtml#ch09qa8" id="rch09qa8">8.</a> B.</strong> Enable EFS Lifecycle Management and choose an age-off policy to move the files to EFS IA, which automatically moves your data to the EFS IA storage class based on the lifecycle policy.</p>
<p class="number"><strong><a href="ch09.xhtml#ch09qa9" id="rch09qa9">9.</a> D.</strong> Amazon S3 Glacier is the best possible solution for the archival storage to save those files in a cost-effective way.</p>
<p class="number1"><strong><a href="ch09.xhtml#ch09qa10" id="rch09qa10">10.</a> C.</strong> There is no maximum or minimum limit, and the individual archive is from 1 byte to 40 terabytes.</p>
<h3 class="h3" id="ch9lev1sec28"><span epub:type="pagebreak" id="page_282"/>Additional Resources</h3>
<p class="bulleta">• <strong>AWS Reference</strong>   There is no place like official AWS documentation to get the latest and most up-to-date information about all the AWS services. Always refer to the official AWS blogs to get the latest updates about new AWS services and updates to existing features.</p>
<p class="bulletc"><img alt="images" src="p0282-01.jpg"/></p>
<p class="bulleta">• <strong>Data Archival Using PowerShell</strong>   This blog explains detailed steps to migrate large amounts of data to Amazon S3 Glacier using the AWS PowerShell.</p>
<p class="bulletc"><img alt="images" src="p0282-02.jpg"/></p>
<p class="bulleta">• <strong>Automating Amazon EBS Snapshots</strong>   This blog explains the detailed steps to automate Amazon EBS snapshots with Amazon CloudWatch events, Amazon Lambda, and AWS Step Functions using AWS CLI.</p>
<p class="bulletc"><img alt="images" src="p0282-03.jpg"/></p>
<p class="bulleta">• <strong>Recovering Files from an Amazon EBS</strong>   This blog shows how to restore an EBS snapshot volume, attach an EBS volume to an EC2 instance, and copy the files to be recovered.</p>
<p class="bulletc"><img alt="images" src="p0282-04.jpg"/></p>
<p class="bulleta">• <strong>Consistent Snapshots of Your Multiple Amazon EBS Volumes</strong>   This blog explains how to create crash-consistent snapshots across all the EBS volumes attached to an EC2 instance using the AWS Command Line Interface and console and automate snapshot management using Amazon DLM.</p>
<p class="bulletc"><img alt="images" src="p0282-05.jpg"/></p>
<p class="bulleta">• <strong>Data Protection Using AWS Backup</strong>   This blog explains the steps to create and maintain backup schedules and monitor AWS Backup jobs.</p>
<p class="bulletc"><img alt="images" src="p0282-06.jpg"/></p>
</section>
</body>
</html>